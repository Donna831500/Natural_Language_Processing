{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\notya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\notya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better display 3 samples, I set the max column width to the max of review string. \n",
    "# This process can show the whole review in the table\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data to a pandas dataframe\n",
    "df = pd.read_table('amazon_reviews_us_Kitchen_v1_00.tsv', error_bad_lines=False,warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Beautiful.  Looks great on counter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I personally have 5 days sets and have also bought 2 sets for other people in my home. The two other sets I have decided to keep for myself. the purpose of keeping them for myself is to use them for other other than salt and pepper. they stay perfect, I use them constantly! I have a couple of people here that use them, say that there's just awesome. I did have a salt shaker that had a little problem converting from sea salt to Himalaya salt.  did not fall out correctly so what I did was I just took a top of part which is really simple it's like five pieces total and just kind of cleaned around the Teflon and you know that the salt buildup from there was caused by humidity,  cleaned out, my gosh it's unbelievable how much better, its almost better than new Thank You Bavaria these are top of the line in my book I hope in the near future you have some more come out.  I sure would like to buy some more and some for my kids and family for Christmas otherwise I'm keeping what I have. I think I deserve it. thank you.... don't know if this makes much sense doesn't need to, self-ness !!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fabulous and worth every penny. Used for cleaning corn from the cob in seconds :) Would recommend its purchase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating  \\\n",
       "0          5.0   \n",
       "1          5.0   \n",
       "2          5.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               review_body  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Beautiful.  Looks great on counter.  \n",
       "1  I personally have 5 days sets and have also bought 2 sets for other people in my home. The two other sets I have decided to keep for myself. the purpose of keeping them for myself is to use them for other other than salt and pepper. they stay perfect, I use them constantly! I have a couple of people here that use them, say that there's just awesome. I did have a salt shaker that had a little problem converting from sea salt to Himalaya salt.  did not fall out correctly so what I did was I just took a top of part which is really simple it's like five pieces total and just kind of cleaned around the Teflon and you know that the salt buildup from there was caused by humidity,  cleaned out, my gosh it's unbelievable how much better, its almost better than new Thank You Bavaria these are top of the line in my book I hope in the near future you have some more come out.  I sure would like to buy some more and some for my kids and family for Christmas otherwise I'm keeping what I have. I think I deserve it. thank you.... don't know if this makes much sense doesn't need to, self-ness !!!  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Fabulous and worth every penny. Used for cleaning corn from the cob in seconds :) Would recommend its purchase  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the sample that I selected, all of them received 5 star ratings.\n"
     ]
    }
   ],
   "source": [
    "# I select the first 3 reviews as samples to show\n",
    "from IPython.display import display\n",
    "raw_data = df[['star_rating','review_body']]\n",
    "show_data = raw_data.head(3)\n",
    "print(\"There are 3 sample reviews:\")\n",
    "display(show_data)\n",
    "print(\"For the sample that I selected, all of them received 5 star ratings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of reviews for class 0, 1 and neutral reviews (rating 3) are 668848, 3856492, and 349547. The average rating for class 0, 1 and neutral reviews (rating 3) are 1.3617383919814368, 3, and 4.810259427479688. \n"
     ]
    }
   ],
   "source": [
    "# Label reviews with rating 4,5 to class 1 and reviews with rating 1,2 to class 0\n",
    "df1 = raw_data[raw_data['star_rating'] > 3].dropna()\n",
    "df2 = raw_data[raw_data['star_rating'] < 3].dropna()\n",
    "df1['label'] = 1\n",
    "df2['label'] = 0\n",
    "\n",
    "# count the number of reviews for each star rating and compute statistics\n",
    "count1 = raw_data[raw_data['star_rating'] == 1].iloc[:,0].size\n",
    "count2 = raw_data[raw_data['star_rating'] == 2].iloc[:,0].size\n",
    "count3 = raw_data[raw_data['star_rating'] == 3].iloc[:,0].size\n",
    "count4 = raw_data[raw_data['star_rating'] == 4].iloc[:,0].size\n",
    "count5 = raw_data[raw_data['star_rating'] == 5].iloc[:,0].size\n",
    "answer_str = 'The number of reviews for class 0, 1 and neutral reviews (rating 3) are '\n",
    "answer_str = answer_str+str(count1+count2)+', '+str(count4+count5)+', and '+str(count3)+'. '\n",
    "\n",
    "avg0 = (count1+(count2*2))/(count1+count2)\n",
    "avg1 = ((count4*4)+(count5*5))/(count4+count5)\n",
    "answer_str = answer_str+'The average rating for class 0, 1 and neutral reviews (rating 3) are '\n",
    "answer_str = answer_str+str(avg0)+', 3, and '+str(avg1)+'. '\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100,000 positive and 100,000 negative reviews. \n",
    "# In order to get same result everytime, the random_state is set to be a constant.\n",
    "# concate positive and negative review dataframe at the end. \n",
    "pos_df = df1.sample(n = 100000, random_state=2)\n",
    "neg_df = df2.sample(n = 100000, random_state=2)\n",
    "all_data = pd.concat([pos_df, neg_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning data, keep a copy of it for future computation\n",
    "before_clean_data = all_data.copy()\n",
    "\n",
    "# convert all reviews to lower case\n",
    "all_data['review_body'] = all_data['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML\n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: BeautifulSoup(text).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URL by remove all word start with 'http:' or 'https:', then remove all word start with 'www.' and end with '.com'\n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: re.sub(r'https?:\\S+', '', text)) \n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: re.sub(r'www.\\S+.com', '', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I manually code the contraction function by replace specific expression with their expand version.\n",
    "def contractionfunction(s):\n",
    "    # specific\n",
    "    s = re.sub(r\"won\\'t\", \"will not\", s)\n",
    "    s = re.sub(r\"can\\'t\", \"can not\", s)\n",
    "    s = re.sub(r'ain\\'t', 'are not', s)\n",
    "\n",
    "    # general\n",
    "    s = re.sub(r\"n\\'t\", \" not\", s)\n",
    "    s = re.sub(r'(\\w+)\\'re', '\\g<1> are', s)\n",
    "    s = re.sub(r'(\\w+)\\'s', '\\g<1> is', s)\n",
    "    s = re.sub(r'(\\w+)\\'d', '\\g<1> would', s)\n",
    "    s = re.sub(r'(\\w+)\\'ll', '\\g<1> will', s)\n",
    "    s = re.sub(r'(\\w+)\\'t', '\\g<1> not', s)\n",
    "    s = re.sub(r'(\\w+)\\'ve', '\\g<1> have', s)\n",
    "    s = re.sub(r'(\\w+)\\'m', '\\g<1> am', s)\n",
    "    return s\n",
    "\n",
    "all_data['review_body'] = all_data['review_body'].apply(contractionfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-alphabetical characters\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: regex.sub(' ', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the extra spaces between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all extra spaces\n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: re.sub(' +', ' ', text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before data cleaning, the average length of review is 324.450075. After data cleaning, the average length of review is 311.069675. \n"
     ]
    }
   ],
   "source": [
    "# compute values and report\n",
    "before_clean = sum(before_clean_data[\"review_body\"].str.len())/(before_clean_data.iloc[:,0].size)\n",
    "after_clean = sum(all_data[\"review_body\"].str.len())/(all_data.iloc[:,0].size)\n",
    "print('Before data cleaning, the average length of review is '+str(before_clean)+'. After data cleaning, the average length of review is '+str(after_clean)+'. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before pre-processing, keep a copy of old data for future computation\n",
    "before_clean_data1 = all_data.copy()\n",
    "\n",
    "# create a stop word list for english\n",
    "from nltk.corpus import stopwords\n",
    "words_list = stopwords.words('english')\n",
    "\n",
    "# split each review into words and check them one by one, remove the word if it is a stop word, \n",
    "# and concate the words finally\n",
    "def remove_stop(s):\n",
    "    pieces = s.split()\n",
    "    result = ''\n",
    "    for each_word in pieces:\n",
    "        if each_word not in words_list:\n",
    "            result = result+' '+each_word\n",
    "    if len(result)>0:\n",
    "        result = result[1:]\n",
    "    return result\n",
    "\n",
    "all_data['review_body'] = all_data['review_body'].apply(remove_stop) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# split each review into words and perform lemmatization one by one, and concate the words finally\n",
    "def lemmatization(s):\n",
    "    pieces = s.split()\n",
    "    result = ''\n",
    "    for each_word in pieces:\n",
    "        temp = lemmatizer.lemmatize(each_word)\n",
    "        result = result+' '+temp\n",
    "    if len(result)>0:\n",
    "        result = result[1:]\n",
    "    return result\n",
    "\n",
    "all_data['review_body'] = all_data['review_body'].apply(lemmatization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 sample reviews before data cleaning and preprocessing:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3407371</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Love it. They are fun to use. Color beautiful. Wonderful addition to your home, but would make a great gift for someone you love.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4135068</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This product replaced an older Cuisinart burr mill,DBM-8(price about $50.00) which served me well for over 5 years but needed a new (non-conical)burr mechanism,which in turn would require shipping and fixed at some expense,leaving me with no grinder for a week or two.For about $100.00 I got the CBM-18N in a couple of days from Amazon. It is less noisy than the DBM-8,is very attractive,and grinds very well.The only coffee brands I use are Allegro and Kaladi Brothers.The darker roasts are oily and tended to clog the older machine. The CMB-18N has not clogged after over a month of use. And I found it very easy to clean a few days ago,(just preventive). This machine is a bargain!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4789448</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I am very satisfied with my purchase, now I just have to find the time to use the product on a regular basis!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating  \\\n",
       "3407371          5.0   \n",
       "4135068          5.0   \n",
       "4789448          4.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          review_body  \\\n",
       "3407371                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Love it. They are fun to use. Color beautiful. Wonderful addition to your home, but would make a great gift for someone you love.   \n",
       "4135068  This product replaced an older Cuisinart burr mill,DBM-8(price about $50.00) which served me well for over 5 years but needed a new (non-conical)burr mechanism,which in turn would require shipping and fixed at some expense,leaving me with no grinder for a week or two.For about $100.00 I got the CBM-18N in a couple of days from Amazon. It is less noisy than the DBM-8,is very attractive,and grinds very well.The only coffee brands I use are Allegro and Kaladi Brothers.The darker roasts are oily and tended to clog the older machine. The CMB-18N has not clogged after over a month of use. And I found it very easy to clean a few days ago,(just preventive). This machine is a bargain!   \n",
       "4789448                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 I am very satisfied with my purchase, now I just have to find the time to use the product on a regular basis!   \n",
       "\n",
       "         label  \n",
       "3407371      1  \n",
       "4135068      1  \n",
       "4789448      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 sample reviews after data cleaning and preprocessing:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3407371</th>\n",
       "      <td>5.0</td>\n",
       "      <td>love fun use color beautiful wonderful addition home would make great gift someone love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4135068</th>\n",
       "      <td>5.0</td>\n",
       "      <td>product replaced older cuisinart burr mill dbm price served well year needed new non conical burr mechanism turn would require shipping fixed expense leaving grinder week two got cbm n couple day amazon le noisy dbm attractive grind well coffee brand use allegro kaladi brother darker roast oily tended clog older machine cmb n clogged month use found easy clean day ago preventive machine bargain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4789448</th>\n",
       "      <td>4.0</td>\n",
       "      <td>satisfied purchase find time use product regular basis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating  \\\n",
       "3407371          5.0   \n",
       "4135068          5.0   \n",
       "4789448          4.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                           review_body  \\\n",
       "3407371                                                                                                                                                                                                                                                                                                                        love fun use color beautiful wonderful addition home would make great gift someone love   \n",
       "4135068  product replaced older cuisinart burr mill dbm price served well year needed new non conical burr mechanism turn would require shipping fixed expense leaving grinder week two got cbm n couple day amazon le noisy dbm attractive grind well coffee brand use allegro kaladi brother darker roast oily tended clog older machine cmb n clogged month use found easy clean day ago preventive machine bargain   \n",
       "4789448                                                                                                                                                                                                                                                                                                                                                         satisfied purchase find time use product regular basis   \n",
       "\n",
       "         label  \n",
       "3407371      1  \n",
       "4135068      1  \n",
       "4789448      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing, the average length of review is 311.069675. After preprocessing, the average length of review is 190.024995. \n"
     ]
    }
   ],
   "source": [
    "# compute values and report\n",
    "show_data_before = before_clean_data.head(3)\n",
    "print(\"There are 3 sample reviews before data cleaning and preprocessing:\")\n",
    "display(show_data_before)\n",
    "show_data_after = all_data.head(3)\n",
    "print(\"There are 3 sample reviews after data cleaning and preprocessing:\")\n",
    "display(show_data_after)\n",
    "\n",
    "before_clean = sum(before_clean_data1[\"review_body\"].str.len())/(before_clean_data1.iloc[:,0].size)\n",
    "after_clean = sum(all_data[\"review_body\"].str.len())/(all_data.iloc[:,0].size)\n",
    "print('Before preprocessing, the average length of review is '+str(before_clean)+'. After preprocessing, the average length of review is '+str(after_clean)+'. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "review_list = all_data['review_body'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(review_list)\n",
    "#print(X.shape)\n",
    "vector_df = pd.DataFrame.sparse.from_spmatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>59768</th>\n",
       "      <th>59769</th>\n",
       "      <th>59770</th>\n",
       "      <th>59771</th>\n",
       "      <th>59772</th>\n",
       "      <th>59773</th>\n",
       "      <th>59774</th>\n",
       "      <th>59775</th>\n",
       "      <th>59776</th>\n",
       "      <th>59777</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows Ã— 59778 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1      2      3      4      5      6      7      8      9      \\\n",
       "0         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "199995    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199996    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199997    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199998    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199999    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "        ...  59768  59769  59770  59771  59772  59773  59774  59775  59776  \\\n",
       "0       ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1       ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2       ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3       ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4       ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "199995  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199996  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199997  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199998  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "199999  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "        59777  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "...       ...  \n",
       "199995    0.0  \n",
       "199996    0.0  \n",
       "199997    0.0  \n",
       "199998    0.0  \n",
       "199999    0.0  \n",
       "\n",
       "[200000 rows x 59778 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test, to keep classes distribute evenly, I set the stratify to label list.\n",
    "from sklearn.model_selection import train_test_split\n",
    "all_label = all_data['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(vector_df,all_label, test_size=0.2,random_state=2,stratify=all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(random_state=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(random_state=2)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Perceptron model, the accuracy, precision, recall and f1-score of training dataset are 0.90518125, 0.8894522473597578, 0.925375, 0.9070580955823342. The accuracy, precision, recall and f1-score of testing dataset are 0.8582, 0.8443237527636259, 0.87835, 0.8610008332108023.\n"
     ]
    }
   ],
   "source": [
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = clf.predict(x_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor Perceptron model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = clf.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(random_state=2)\n",
    "lsvc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For SVM model, the accuracy, precision, recall and f1-score of training dataset are 0.93365, 0.934497269675868, 0.932675, 0.9335852456144742. The accuracy, precision, recall and f1-score of testing dataset are 0.895375, 0.8987846084018357, 0.8911, 0.8949258078284666.\n"
     ]
    }
   ],
   "source": [
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = lsvc.predict(x_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor SVM model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = lsvc.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Logistic Regression model, the accuracy, precision, recall and f1-score of training dataset are 0.91461875, 0.9173996753369323, 0.9112875, 0.9143333730489693. The accuracy, precision, recall and f1-score of testing dataset are 0.8989, 0.9045638945233265, 0.8919, 0.8981873111782477.\n"
     ]
    }
   ],
   "source": [
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = clf.predict(x_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor Logistic Regression model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = clf.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Multinomial Naive Bayes model, the accuracy, precision, recall and f1-score of training dataset are 0.88716875, 0.8927407595257719, 0.880075, 0.886362634941617. The accuracy, precision, recall and f1-score of testing dataset are 0.867275, 0.8749425756725027, 0.85705, 0.865903867040489.\n"
     ]
    }
   ],
   "source": [
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = clf.predict(x_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor Multinomial Naive Bayes model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = clf.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
