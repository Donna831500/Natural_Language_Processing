{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXC20as21Nk1",
    "outputId": "4ad60b1b-fa37-4ee4-fed5-61524fe5655b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39fvpLdX7_xp",
    "outputId": "366d42e0-3b42-450f-a6bd-5a490f069f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FyccSIQ1GQw"
   },
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9B1SYC90m8d"
   },
   "outputs": [],
   "source": [
    "# read the data to a pandas dataframe, to save the RAM, only read two columns that we need\n",
    "df = pd.read_table('amazon_reviews_us_Kitchen_v1_00.tsv',\n",
    "                   usecols = ['star_rating','review_body'],\n",
    "                   error_bad_lines=False,warn_bad_lines=False).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyfF5iZu9VGd"
   },
   "outputs": [],
   "source": [
    "# Select 50K instances for each rating score\n",
    "df1 = df[df['star_rating'] == 1]\n",
    "df2 = df[df['star_rating'] == 2]\n",
    "df3 = df[df['star_rating'] == 3]\n",
    "df4 = df[df['star_rating'] == 4]\n",
    "df5 = df[df['star_rating'] == 5]\n",
    "\n",
    "SAMPLE_NUM = 50000\n",
    "\n",
    "df1_sample = df1.sample(n = SAMPLE_NUM) \n",
    "df2_sample = df2.sample(n = SAMPLE_NUM)\n",
    "df3_sample = df3.sample(n = SAMPLE_NUM)\n",
    "df4_sample = df4.sample(n = SAMPLE_NUM)\n",
    "df5_sample = df5.sample(n = SAMPLE_NUM)\n",
    "\n",
    "all_data = pd.concat([df1_sample, df2_sample,df3_sample,df4_sample,df5_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHmAHxN3Y3cS"
   },
   "outputs": [],
   "source": [
    "# add ternary label \n",
    "def convert_label(s):\n",
    "    if s<3:\n",
    "        return 0;\n",
    "    elif s==3:\n",
    "        return 1;\n",
    "    else:\n",
    "        return 2;\n",
    "\n",
    "all_data['label'] = all_data['star_rating'].apply(convert_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRfLPqvtZWVX"
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jj3rKNu22pZ4"
   },
   "outputs": [],
   "source": [
    "# Perform Data Cleaning\n",
    "\n",
    "# convert all reviews to lower case\n",
    "all_data['review_body'] = all_data['review_body'].str.lower()\n",
    "\n",
    "# Remove HTML\n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: BeautifulSoup(text).get_text())\n",
    "\n",
    "# Remove URL by remove all word start with 'http:' or 'https:', then remove all word start with 'www.' and end with '.com'\n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: re.sub(r'https?:\\S+', '', text)) \n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: re.sub(r'www.\\S+.com', '', text))\n",
    "\n",
    "# I manually code the contraction function by replace specific expression with their expand version.\n",
    "def contractionfunction(s):\n",
    "    # specific\n",
    "    s = re.sub(r\"won\\'t\", \"will not\", s)\n",
    "    s = re.sub(r\"can\\'t\", \"can not\", s)\n",
    "    s = re.sub(r'ain\\'t', 'are not', s)\n",
    "\n",
    "    # general\n",
    "    s = re.sub(r\"n\\'t\", \" not\", s)\n",
    "    s = re.sub(r'(\\w+)\\'re', '\\g<1> are', s)\n",
    "    s = re.sub(r'(\\w+)\\'s', '\\g<1> is', s)\n",
    "    s = re.sub(r'(\\w+)\\'d', '\\g<1> would', s)\n",
    "    s = re.sub(r'(\\w+)\\'ll', '\\g<1> will', s)\n",
    "    s = re.sub(r'(\\w+)\\'t', '\\g<1> not', s)\n",
    "    s = re.sub(r'(\\w+)\\'ve', '\\g<1> have', s)\n",
    "    s = re.sub(r'(\\w+)\\'m', '\\g<1> am', s)\n",
    "    return s\n",
    "\n",
    "all_data['review_body'] = all_data['review_body'].apply(contractionfunction)\n",
    "\n",
    "# remove all non-alphabetical characters\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: regex.sub(' ', text))\n",
    "\n",
    "# Remove all extra spaces\n",
    "all_data['review_body'] = all_data['review_body'].apply(lambda text: re.sub(' +', ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L4yifnob4N6"
   },
   "outputs": [],
   "source": [
    "# Clear unnecessary variables to release RAM\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtY97hU2Q2Mr"
   },
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA_1vArb80q"
   },
   "source": [
    "#### part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDZo9OcBQ-5N"
   },
   "outputs": [],
   "source": [
    "# Load 'word2vec-google-news-300' pretrained model\n",
    "import gensim.downloader as api\n",
    "wv_g = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMA3YWKz40rv"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# wv_g.save(\"word2vec.google_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNHgI80lW4rE",
    "outputId": "13221137-014f-4250-945a-98545a1f70ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: According to Pretrained Model, the similarity between word 'good' and 'nice' is 0.6836092\n",
      "Similar words have high similarity.\n"
     ]
    }
   ],
   "source": [
    "w1 = 'good'\n",
    "w2 = 'nice'\n",
    "print(\"Example 1: According to Pretrained Model, the similarity between word '\"+w1+\"' and '\"+w2+\"' is \"+str(wv_g.similarity(w1, w2)))\n",
    "print(\"Similar words have high similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ob79eFbVbmPE",
    "outputId": "5beb76dd-c18f-43fd-eff0-2b0fed0f153b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2: According to Pretrained Model, the similarity between word 'big' and 'large' is 0.5561479\n",
      "Similar words have high similarity.\n"
     ]
    }
   ],
   "source": [
    "w1 = 'big'\n",
    "w2 = 'large'\n",
    "print(\"Example 2: According to Pretrained Model, the similarity between word '\"+w1+\"' and '\"+w2+\"' is \"+str(wv_g.similarity(w1, w2)))\n",
    "print(\"Similar words have high similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1z6cYvvbxxy",
    "outputId": "cd336d6e-3bad-41be-af62-d0694d750008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 most similar words and their similarity with 'King'-'Man'+'Woman':\n",
      "[('king', 0.8449392318725586), ('queen', 0.7300517559051514), ('monarch', 0.6454660892486572), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676948547363)]\n",
      "\n",
      "This example shows that 'King'-'Man'+'Woman'= Queen \n"
     ]
    }
   ],
   "source": [
    "vec_king = wv_g['king']\n",
    "vec_man = wv_g['man']\n",
    "vec_woman = wv_g['woman']\n",
    "vec_queen = wv_g['queen']\n",
    "print(\"There are 5 most similar words and their similarity with 'King'-'Man'+'Woman':\")\n",
    "print(wv_g.similar_by_vector((vec_king-vec_man+vec_woman), topn=5, restrict_vocab=None))\n",
    "print(\"\\nThis example shows that 'King'-'Man'+'Woman'= Queen \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6WMnjXKcAga"
   },
   "source": [
    "#### part (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTPKO1Gd5Wzk"
   },
   "outputs": [],
   "source": [
    "# Train My Model\n",
    "\n",
    "# Format all reviews into list of list of words for future training\n",
    "from gensim.models import Word2Vec\n",
    "raw_sentences = all_data['review_body'].tolist()\n",
    "all_sentences = []\n",
    "for each_s in raw_sentences:\n",
    "  temp = each_s.split()\n",
    "  all_sentences.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcrSyL-uCa6c",
    "outputId": "30db81db-eb24-42a7-f900-663a74d17847"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169562282, 238679070)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train My model\n",
    "mymodel = Word2Vec(size=300, window=11, min_count=10, workers=4)\n",
    "mymodel.build_vocab(all_sentences)  \n",
    "mymodel.train(sentences=all_sentences, total_examples=mymodel.corpus_count, epochs=15)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36B4nHgnCa9E",
    "outputId": "3fde0e79-39ae-450e-ccd7-54d85b90f8d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: According to My Model, the similarity between word 'good' and 'nice' is 0.6280225\n",
      "Similar words have high similarity.\n"
     ]
    }
   ],
   "source": [
    "w1 = 'good'\n",
    "w2 = 'nice'\n",
    "print(\"Example 1: According to My Model, the similarity between word '\"+w1+\"' and '\"+w2+\"' is \"+str(mymodel.similarity(w1, w2)))\n",
    "print(\"Similar words have high similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jItRZ7EMEap8",
    "outputId": "8cb76582-00e0-444d-f535-97e08de51227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2: According to My Model, the similarity between word 'big' and 'large' is 0.62499684\n",
      "Similar words have high similarity.\n"
     ]
    }
   ],
   "source": [
    "w1 = 'big'\n",
    "w2 = 'large'\n",
    "print(\"Example 2: According to My Model, the similarity between word '\"+w1+\"' and '\"+w2+\"' is \"+str(mymodel.similarity(w1, w2)))\n",
    "print(\"Similar words have high similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XnchFkRf6GX",
    "outputId": "de036168-c3bb-4ea1-e12a-52dff0be698a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For my model, there are 5 most similar words and their similarity with 'King'-'Man'+'Woman':\n",
      "[('king', 0.5912432074546814), ('woman', 0.4643744230270386), ('petite', 0.3288615942001343), ('arthur', 0.32821086049079895), ('textured', 0.32040905952453613)]\n",
      "\n",
      "This example does not show that 'King'-'Man'+'Woman'= 'Queen', \n",
      "This is because the those word are not show in reviews frequently, so we do not have enough data to train the model.\n"
     ]
    }
   ],
   "source": [
    "vec_king = mymodel.wv['king']\n",
    "vec_man = mymodel.wv['man']\n",
    "vec_woman = mymodel.wv['woman']\n",
    "vec_queen = mymodel.wv['queen']\n",
    "print(\"For my model, there are 5 most similar words and their similarity with 'King'-'Man'+'Woman':\")\n",
    "print(mymodel.similar_by_vector((vec_king-vec_man+vec_woman), topn=5, restrict_vocab=None))\n",
    "print(\"\\nThis example does not show that 'King'-'Man'+'Woman'= 'Queen', \")\n",
    "print(\"This is because the those word are not show in reviews frequently, so we do not have enough data to train the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQZZhp413nfe"
   },
   "outputs": [],
   "source": [
    "mymodel.save(\"word2vec.mymodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Zj32XNfjd0x",
    "outputId": "97461f56-f9da-45d9-e7c6-26c9690eeb61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the above example result we can see that, the performance of comparing high frequent words between pretrained model and my model are similar.\n",
      "However, for low frequency word, the pretrained model perform better.\n",
      "This is resonable because those 'low frequent' words appears more in training data of pretrained model.\n"
     ]
    }
   ],
   "source": [
    "print(\"From the above example result we can see that, the performance of comparing high frequent words between pretrained model and my model are similar.\")\n",
    "print(\"However, for low frequency word, the pretrained model perform better.\")\n",
    "print(\"This is resonable because those 'low frequent' words appears more in training data of pretrained model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1FK1wh8gj2i"
   },
   "outputs": [],
   "source": [
    "# Clear unnecessary variables to release RAM\n",
    "del all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lypwM8JE8TkB"
   },
   "source": [
    "## 3. Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpl--PBsj4gg"
   },
   "source": [
    "#### ------------Pre-processing----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4GaGYegjJMv"
   },
   "outputs": [],
   "source": [
    "# remove the stop words \n",
    "clean_data = all_data.copy()\n",
    "# create a stop word list for english\n",
    "from nltk.corpus import stopwords\n",
    "words_list = stopwords.words('english')\n",
    "\n",
    "# split each review into words and check them one by one, remove the word if it is a stop word, \n",
    "# and concate the words finally\n",
    "def remove_stop(s):\n",
    "    pieces = s.split()\n",
    "    result = ''\n",
    "    for each_word in pieces:\n",
    "        if each_word not in words_list:\n",
    "            result = result+' '+each_word\n",
    "    if len(result)>0:\n",
    "        result = result[1:]\n",
    "    return result\n",
    "\n",
    "clean_data['review_body'] = clean_data['review_body'].apply(remove_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJiFIndZjRYQ"
   },
   "outputs": [],
   "source": [
    "# create a dataframe for binary data\n",
    "binary_data = clean_data[clean_data['star_rating']!=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH9fv3YyrpRa"
   },
   "source": [
    "####TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OapNRxagi-Ic"
   },
   "outputs": [],
   "source": [
    "# TF-IDF Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "review_list = binary_data['review_body'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(review_list)\n",
    "#print(X.shape)\n",
    "vector_df = pd.DataFrame.sparse.from_spmatrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGFVF4kHsiZ5"
   },
   "outputs": [],
   "source": [
    "# split train and test, to keep classes distribute evenly, I set the stratify to label list.\n",
    "from sklearn.model_selection import train_test_split\n",
    "binary_label = binary_data['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(vector_df,binary_label, test_size=0.2,random_state=2,stratify=binary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rpkg9SRg11T1",
    "outputId": "44086332-203b-470f-e8d4-ca1d51227da2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For TF-IDF + Perceptron model, the accuracy, precision, recall and f1-score of training dataset are 0.982375, 0.9790218470705064, 0.985875, 0.9824364723467862. The accuracy, precision, recall and f1-score of testing dataset are 0.806, 0.792822966507177, 0.8285, 0.8102689486552567.\n"
     ]
    }
   ],
   "source": [
    "# Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(random_state=2)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = clf.predict(x_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor TF-IDF + Perceptron model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = clf.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aH-s4iLVEHsz",
    "outputId": "8cea9561-5f0e-4cc2-e0c3-f355316fa21e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For TF-IDF + SVM model, the accuracy, precision, recall and f1-score of training dataset are 0.9651875, 0.965944660072618, 0.964375, 0.9651591918433727. The accuracy, precision, recall and f1-score of testing dataset are 0.8445, 0.843812375249501, 0.8455, 0.8446553446553448.\n"
     ]
    }
   ],
   "source": [
    "# --------------------SVM----------------------------\n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(random_state=2)\n",
    "lsvc.fit(x_train, y_train)\n",
    "\n",
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = lsvc.predict(x_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor TF-IDF + SVM model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = lsvc.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8dCcUW10l5U"
   },
   "source": [
    "#### prepare train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzuDbTiA0iQK"
   },
   "outputs": [],
   "source": [
    "def compute_avg(s,input_model):\n",
    "  words = s.split()\n",
    "  num_of_words = len(words)\n",
    "  if num_of_words==0:\n",
    "    return [0]*300\n",
    "  else:\n",
    "    current_total_vector = [0]*300\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        word_vector = input_model[each_word]\n",
    "        current_total_vector = [a+b for a,b in zip(current_total_vector,word_vector)]\n",
    "      except:\n",
    "        pass\n",
    "    avg_vector = [x / num_of_words for x in current_total_vector]\n",
    "    return avg_vector\n",
    "\n",
    "def get_avg_W2V(input_model1,df1):\n",
    "  temp_df = df1[['review_body']].copy()\n",
    "  temp_df['all_vec'] = temp_df.apply(lambda y: compute_avg(y['review_body'],input_model1),axis = 1)\n",
    "  vec_df = temp_df['all_vec'].apply(pd.Series)\n",
    "  return vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_Y0dQnAXzTS"
   },
   "outputs": [],
   "source": [
    "# Compute average and Split ternary data into train and test part for 2 Word2Vec models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ternary_label = clean_data['label']\n",
    "\n",
    "# my model\n",
    "my_vec_df_ternary = get_avg_W2V(mymodel.wv,clean_data)\n",
    "x_train_3_my, x_test_3_my, y_train_3_my, y_test_3_my = train_test_split(my_vec_df_ternary,ternary_label, test_size=0.2,random_state=2,stratify=ternary_label)\n",
    "\n",
    "# pretrained model\n",
    "g_vec_df_ternary = get_avg_W2V(wv_g,clean_data)\n",
    "x_train_3_g, x_test_3_g, y_train_3_g, y_test_3_g = train_test_split(g_vec_df_ternary,ternary_label, test_size=0.2,random_state=2,stratify=ternary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qHAiMKschNQ"
   },
   "outputs": [],
   "source": [
    "# Compute average and Split Binary data into train and test part for 2 Word2Vec models\n",
    "\n",
    "# my model\n",
    "my_data_df_ternary = my_vec_df_ternary.copy()\n",
    "my_data_df_ternary['label'] = ternary_label\n",
    "my_data_df_binary = my_data_df_ternary[my_data_df_ternary['label']!=1]\n",
    "my_vec_df_binary = my_data_df_binary.iloc[:, :-1].values\n",
    "my_binary_label = my_data_df_binary.iloc[:, 300].values\n",
    "my_binary_label = np.where(my_binary_label == 2, 1, my_binary_label)\n",
    "x_train_2_my, x_test_2_my, y_train_2_my, y_test_2_my = train_test_split(my_vec_df_binary,my_binary_label, test_size=0.2,random_state=2,stratify=my_binary_label)\n",
    "\n",
    "# pretrained model\n",
    "g_data_df_ternary = g_vec_df_ternary.copy()\n",
    "g_data_df_ternary['label'] = ternary_label\n",
    "g_data_df_binary = g_data_df_ternary[g_data_df_ternary['label']!=1]\n",
    "g_vec_df_binary = g_data_df_binary.iloc[:, :-1].values\n",
    "g_binary_label = g_data_df_binary.iloc[:, 300].values\n",
    "g_binary_label = np.where(g_binary_label == 2, 1, g_binary_label)\n",
    "x_train_2_g, x_test_2_g, y_train_2_g, y_test_2_g = train_test_split(g_vec_df_binary,g_binary_label, test_size=0.2,random_state=2,stratify=g_binary_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTHU-B-gZlTX"
   },
   "source": [
    "#### My Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mI16Dsj5LbvT",
    "outputId": "b08352c3-89ee-4a77-e13a-30774f845e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For my Word2Vec + Perceptron model, the accuracy, precision, recall and f1-score of training dataset are 0.73401875, 0.8380003249742729, 0.5802, 0.6856687027749669. The accuracy, precision, recall and f1-score of testing dataset are 0.73325, 0.839618520675597, 0.57665, 0.6837206544937159.\n"
     ]
    }
   ],
   "source": [
    "# Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(random_state=2)\n",
    "clf.fit(x_train_2_my, y_train_2_my)\n",
    "\n",
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = clf.predict(x_train_2_my)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_2_my, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor my Word2Vec + Perceptron model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = clf.predict(x_test_2_my)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2_my, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqwNgBQhLiMu",
    "outputId": "450e4994-905d-4ab5-fc84-51f3b11c715c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For my Word2Vec + SVM model, the accuracy, precision, recall and f1-score of training dataset are 0.84928125, 0.8577189456300488, 0.8374875, 0.8474824966954014. The accuracy, precision, recall and f1-score of testing dataset are 0.849475, 0.859338851472932, 0.83575, 0.8473802945425972.\n"
     ]
    }
   ],
   "source": [
    "# --------------------SVM---------------------------- \n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(random_state=2)\n",
    "lsvc.fit(x_train_2_my, y_train_2_my)\n",
    "\n",
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = lsvc.predict(x_train_2_my)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_2_my, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor my Word2Vec + SVM model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = lsvc.predict(x_test_2_my)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2_my, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s884BtY00rgd"
   },
   "source": [
    "#### pre_trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUv3_eFGMwAw",
    "outputId": "feb4ed6f-ada5-4adc-bf20-f9289c92ca2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For pretrained Word2Vec + Perceptron model, the accuracy, precision, recall and f1-score of training dataset are 0.665025, 0.6025732666190136, 0.96945, 0.743201027272553. The accuracy, precision, recall and f1-score of testing dataset are 0.663975, 0.601705690804776, 0.9701, 0.7427313618528089.\n"
     ]
    }
   ],
   "source": [
    "# Perceptron            \n",
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(random_state=2)\n",
    "clf.fit(x_train_2_g, y_train_2_g)\n",
    "\n",
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = clf.predict(x_train_2_g)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_2_g, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor pretrained Word2Vec + Perceptron model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = clf.predict(x_test_2_g)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2_g, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVAOzVe2MwDG",
    "outputId": "a7cbbd7f-98f7-4dd1-9c63-b88776fc3585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For pretrained Word2Vec + SVM model, the accuracy, precision, recall and f1-score of training dataset are 0.82218125, 0.83827022770523, 0.7984, 0.8178494830180224. The accuracy, precision, recall and f1-score of testing dataset are 0.820325, 0.8358232426482152, 0.79725, 0.8160810707065537.\n"
     ]
    }
   ],
   "source": [
    "# --------------------SVM---------------------------- \n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(random_state=2)\n",
    "lsvc.fit(x_train_2_g, y_train_2_g)\n",
    "\n",
    "# train + test prediction\n",
    "\n",
    "# train\n",
    "y_pred = lsvc.predict(x_train_2_g)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_2_g, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = '\\nFor pretrained Word2Vec + SVM model, the accuracy, precision, recall and f1-score of training dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\". \"\n",
    "\n",
    "# test\n",
    "y_pred = lsvc.predict(x_test_2_g)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2_g, y_pred).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "answer_str = answer_str+'The accuracy, precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(accuracy)+\", \"+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqKDwrHBngRb"
   },
   "source": [
    "###### Conclusion for Q uestion 3:\n",
    "In homework 1, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0Fgl--AQg_3"
   },
   "source": [
    "## 4. Feedforward Neutral Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8EBRO8xQhUW"
   },
   "source": [
    "#### part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJc8ELAdbrzF"
   },
   "outputs": [],
   "source": [
    "# import everything and set device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import torchvision.datasets as datasets # standard datasets\n",
    "import torchvision.transforms as transforms # data processing\n",
    "#import torch.utils.data.TensorDataset #as TensorDataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsGEB6aF3UFX"
   },
   "source": [
    "#### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzjLsLHTillW"
   },
   "outputs": [],
   "source": [
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "\n",
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) #torch.relu()\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        #x = torch.round(x)\n",
    "        #x = torch.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc2i8XdvkZrj"
   },
   "source": [
    "#### Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0z924qebr1d"
   },
   "outputs": [],
   "source": [
    "# Split train data into train and valid data\n",
    "x_puretrain_2_g, x_valid_2_g, y_puretrain_2_g, y_valid_2_g = train_test_split(x_train_2_g,y_train_2_g, test_size=0.2,random_state=2,stratify=y_train_2_g)\n",
    "\n",
    "# convert dataframe to dataset\n",
    "## train data     \n",
    "train_data = trainData(torch.tensor(x_puretrain_2_g), torch.tensor(y_puretrain_2_g))\n",
    "\n",
    "## validation data\n",
    "validation_data = trainData(torch.tensor(x_valid_2_g), torch.tensor(y_valid_2_g))\n",
    "\n",
    "## test data    \n",
    "test_data = testData(torch.tensor(x_test_2_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-L6l781wFqB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_data, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPkW_SKfwYk_"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "b_model = binaryClassification()\n",
    "#b_model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(b_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjbBbQAwzAGZ",
    "outputId": "d54afb3b-dbea-4257-cf02-030fc5cd7f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.398327).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.42731 | Validation Loss: 0.39833\n",
      "Validation loss decreased (0.398327 --> 0.384852).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.39108 | Validation Loss: 0.38485\n",
      "Validation loss decreased (0.384852 --> 0.374852).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.37920 | Validation Loss: 0.37485\n",
      "Validation loss decreased (0.374852 --> 0.372472).  Saving model ...\n",
      "Epoch 003: | Train Loss: 0.37076 | Validation Loss: 0.37247\n",
      "Validation loss decreased (0.372472 --> 0.363599).  Saving model ...\n",
      "Epoch 004: | Train Loss: 0.36345 | Validation Loss: 0.36360\n",
      "Validation loss decreased (0.363599 --> 0.359321).  Saving model ...\n",
      "Epoch 005: | Train Loss: 0.35664 | Validation Loss: 0.35932\n",
      "Validation loss decreased (0.359321 --> 0.359084).  Saving model ...\n",
      "Epoch 006: | Train Loss: 0.35110 | Validation Loss: 0.35908\n",
      "Validation loss decreased (0.359084 --> 0.352811).  Saving model ...\n",
      "Epoch 007: | Train Loss: 0.34577 | Validation Loss: 0.35281\n",
      "Validation loss decreased (0.352811 --> 0.352587).  Saving model ...\n",
      "Epoch 008: | Train Loss: 0.34123 | Validation Loss: 0.35259\n",
      "Epoch 009: | Train Loss: 0.33661 | Validation Loss: 0.35358\n",
      "Validation loss decreased (0.352587 --> 0.348705).  Saving model ...\n",
      "Epoch 010: | Train Loss: 0.33311 | Validation Loss: 0.34870\n",
      "Validation loss decreased (0.348705 --> 0.347166).  Saving model ...\n",
      "Epoch 011: | Train Loss: 0.32911 | Validation Loss: 0.34717\n",
      "Epoch 012: | Train Loss: 0.32584 | Validation Loss: 0.35223\n",
      "Epoch 013: | Train Loss: 0.32238 | Validation Loss: 0.34965\n",
      "Epoch 014: | Train Loss: 0.31909 | Validation Loss: 0.35101\n",
      "Validation loss decreased (0.347166 --> 0.347003).  Saving model ...\n",
      "Epoch 015: | Train Loss: 0.31650 | Validation Loss: 0.34700\n",
      "Validation loss decreased (0.347003 --> 0.344953).  Saving model ...\n",
      "Epoch 016: | Train Loss: 0.31384 | Validation Loss: 0.34495\n",
      "Epoch 017: | Train Loss: 0.31112 | Validation Loss: 0.34517\n",
      "Validation loss decreased (0.344953 --> 0.343057).  Saving model ...\n",
      "Epoch 018: | Train Loss: 0.30893 | Validation Loss: 0.34306\n",
      "Epoch 019: | Train Loss: 0.30657 | Validation Loss: 0.34794\n",
      "Epoch 020: | Train Loss: 0.30399 | Validation Loss: 0.34674\n",
      "Epoch 021: | Train Loss: 0.30214 | Validation Loss: 0.35291\n",
      "Epoch 022: | Train Loss: 0.29978 | Validation Loss: 0.36695\n",
      "Epoch 023: | Train Loss: 0.29865 | Validation Loss: 0.34857\n",
      "Epoch 024: | Train Loss: 0.29642 | Validation Loss: 0.35418\n",
      "Epoch 025: | Train Loss: 0.29496 | Validation Loss: 0.35011\n",
      "Epoch 026: | Train Loss: 0.29320 | Validation Loss: 0.34722\n",
      "Epoch 027: | Train Loss: 0.29190 | Validation Loss: 0.35326\n",
      "Epoch 028: | Train Loss: 0.29023 | Validation Loss: 0.35116\n",
      "Epoch 029: | Train Loss: 0.28884 | Validation Loss: 0.35166\n",
      "Epoch 030: | Train Loss: 0.28726 | Validation Loss: 0.35623\n",
      "Epoch 031: | Train Loss: 0.28604 | Validation Loss: 0.35603\n",
      "Epoch 032: | Train Loss: 0.28514 | Validation Loss: 0.35486\n",
      "Epoch 033: | Train Loss: 0.28321 | Validation Loss: 0.35529\n",
      "Epoch 034: | Train Loss: 0.28272 | Validation Loss: 0.35403\n",
      "Epoch 035: | Train Loss: 0.28137 | Validation Loss: 0.35530\n",
      "Epoch 036: | Train Loss: 0.28032 | Validation Loss: 0.36398\n",
      "Epoch 037: | Train Loss: 0.27882 | Validation Loss: 0.36549\n",
      "Epoch 038: | Train Loss: 0.27783 | Validation Loss: 0.35694\n",
      "Epoch 039: | Train Loss: 0.27689 | Validation Loss: 0.35979\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "N_EPOCHS = 40\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    b_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      X_batch = X_batch.float()\n",
    "      #X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      y_pred = b_model(X_batch)\n",
    "      loss = criterion(y_pred.flatten(), y_batch.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    b_model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        output = b_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.flatten(), target.float())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    if (valid_loss/len(valid_loader)) <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss/len(valid_loader)))\n",
    "        torch.save(b_model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss/len(valid_loader)\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_loss/len(train_loader):.5f} | Validation Loss: {valid_loss/len(valid_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvvSbRJKuFqP",
    "outputId": "ac8dab78-1805-4fc9-a99b-1a9a5b903fd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state of model\n",
    "b_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEPnZvYkxBg6"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        output = model(X_batch)\n",
    "        prediction_list.append(output.item())\n",
    "    \n",
    "    return [round(num) for num in prediction_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OndcDm_dxBo1",
    "outputId": "c08f8d64-09ed-4615-83e9-360014a7a8e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Binary data + Pretrained model, the accuracy for feedforward neural network is 0.84775\n",
      "Other useful values are shown below:\n",
      "The precision, recall and f1-score of testing dataset are 0.8488663723916533, 0.84615, 0.8475060096153845.\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "y_pred = predict(b_model, test_loader)  \n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2_g, np.array(y_pred)).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "print(\"For Binary data + Pretrained model, the accuracy for feedforward neural network is \"+str(accuracy))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Other useful values are shown below:\")\n",
    "answer_str = 'The precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRf09xZz1TkQ"
   },
   "source": [
    "#### My model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDNrSmdb1ZwA"
   },
   "outputs": [],
   "source": [
    "# split training data into train and valid \n",
    "x_puretrain_2_my, x_valid_2_my, y_puretrain_2_my, y_valid_2_my = train_test_split(x_train_2_my,y_train_2_my, test_size=0.2,random_state=2,stratify=y_train_2_my)\n",
    "\n",
    "## train data     \n",
    "train_data = trainData(torch.tensor(x_puretrain_2_my), torch.tensor(y_puretrain_2_my))\n",
    "\n",
    "## validation data\n",
    "validation_data = trainData(torch.tensor(x_valid_2_my), torch.tensor(y_valid_2_my))\n",
    "\n",
    "## test data    \n",
    "test_data = testData(torch.tensor(x_test_2_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIxujpEY1ZwB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_data, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqdZXTL81ZwB"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "b_model = binaryClassification()\n",
    "b_model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(b_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTGXREdy1Xq5",
    "outputId": "440a1fdb-589f-4343-e75a-c312c8da321d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.332100).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.36298 | Validation Loss: 0.33210\n",
      "Validation loss decreased (0.332100 --> 0.320255).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.32838 | Validation Loss: 0.32026\n",
      "Validation loss decreased (0.320255 --> 0.314852).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.31541 | Validation Loss: 0.31485\n",
      "Epoch 003: | Train Loss: 0.30576 | Validation Loss: 0.31497\n",
      "Validation loss decreased (0.314852 --> 0.307469).  Saving model ...\n",
      "Epoch 004: | Train Loss: 0.29751 | Validation Loss: 0.30747\n",
      "Validation loss decreased (0.307469 --> 0.306802).  Saving model ...\n",
      "Epoch 005: | Train Loss: 0.29061 | Validation Loss: 0.30680\n",
      "Epoch 006: | Train Loss: 0.28493 | Validation Loss: 0.31264\n",
      "Validation loss decreased (0.306802 --> 0.306381).  Saving model ...\n",
      "Epoch 007: | Train Loss: 0.27962 | Validation Loss: 0.30638\n",
      "Epoch 008: | Train Loss: 0.27470 | Validation Loss: 0.30797\n",
      "Validation loss decreased (0.306381 --> 0.304983).  Saving model ...\n",
      "Epoch 009: | Train Loss: 0.27037 | Validation Loss: 0.30498\n",
      "Epoch 010: | Train Loss: 0.26669 | Validation Loss: 0.30924\n",
      "Epoch 011: | Train Loss: 0.26357 | Validation Loss: 0.30884\n",
      "Epoch 012: | Train Loss: 0.26005 | Validation Loss: 0.30882\n",
      "Epoch 013: | Train Loss: 0.25738 | Validation Loss: 0.30962\n",
      "Epoch 014: | Train Loss: 0.25397 | Validation Loss: 0.31502\n",
      "Epoch 015: | Train Loss: 0.25180 | Validation Loss: 0.31472\n",
      "Epoch 016: | Train Loss: 0.24927 | Validation Loss: 0.31532\n",
      "Epoch 017: | Train Loss: 0.24693 | Validation Loss: 0.31777\n",
      "Epoch 018: | Train Loss: 0.24497 | Validation Loss: 0.31890\n",
      "Epoch 019: | Train Loss: 0.24274 | Validation Loss: 0.31893\n",
      "Epoch 020: | Train Loss: 0.24069 | Validation Loss: 0.32400\n",
      "Epoch 021: | Train Loss: 0.23903 | Validation Loss: 0.32624\n",
      "Epoch 022: | Train Loss: 0.23712 | Validation Loss: 0.32254\n",
      "Epoch 023: | Train Loss: 0.23568 | Validation Loss: 0.32811\n",
      "Epoch 024: | Train Loss: 0.23388 | Validation Loss: 0.33044\n",
      "Epoch 025: | Train Loss: 0.23196 | Validation Loss: 0.33436\n",
      "Epoch 026: | Train Loss: 0.23102 | Validation Loss: 0.33305\n",
      "Epoch 027: | Train Loss: 0.22951 | Validation Loss: 0.33302\n",
      "Epoch 028: | Train Loss: 0.22786 | Validation Loss: 0.33406\n",
      "Epoch 029: | Train Loss: 0.22696 | Validation Loss: 0.33773\n",
      "Epoch 030: | Train Loss: 0.22567 | Validation Loss: 0.34161\n",
      "Epoch 031: | Train Loss: 0.22449 | Validation Loss: 0.33618\n",
      "Epoch 032: | Train Loss: 0.22327 | Validation Loss: 0.33703\n",
      "Epoch 033: | Train Loss: 0.22210 | Validation Loss: 0.34425\n",
      "Epoch 034: | Train Loss: 0.22081 | Validation Loss: 0.34343\n",
      "Epoch 035: | Train Loss: 0.21996 | Validation Loss: 0.35135\n",
      "Epoch 036: | Train Loss: 0.21823 | Validation Loss: 0.34621\n",
      "Epoch 037: | Train Loss: 0.21721 | Validation Loss: 0.34802\n",
      "Epoch 038: | Train Loss: 0.21657 | Validation Loss: 0.35122\n",
      "Epoch 039: | Train Loss: 0.21553 | Validation Loss: 0.35683\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "N_EPOCHS = 40\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    b_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      X_batch = X_batch.float()\n",
    "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      y_pred = b_model(X_batch)\n",
    "      loss = criterion(y_pred.flatten(), y_batch.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    b_model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = b_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.flatten(), target.float())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    if (valid_loss/len(valid_loader)) <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss/len(valid_loader)))\n",
    "        torch.save(b_model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss/len(valid_loader)\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_loss/len(train_loader):.5f} | Validation Loss: {valid_loss/len(valid_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Cq9tXUX1pQs",
    "outputId": "f8dd867c-037d-496c-f884-a9379b1974bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYVScnjX1pQs"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        X_batch = X_batch.to(device)\n",
    "        output = model(X_batch)\n",
    "        prediction_list.append(output.item())\n",
    "    \n",
    "    return [round(num) for num in prediction_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ml_grpZ91pQs",
    "outputId": "d29014fe-8929-45fb-e216-777bdd30ac00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Binary data + My model, the accuracy for feedforward neural network is 0.86965\n",
      "Other useful values are shown below:\n",
      "The precision, recall and f1-score of testing dataset are 0.8695021991203519, 0.86985, 0.8696760647870425.\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "y_pred = predict(b_model, test_loader)  \n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2_my, np.array(y_pred)).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "print(\"For Binary data + My model, the accuracy for feedforward neural network is \"+str(accuracy))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Other useful values are shown below:\")\n",
    "answer_str = 'The precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsXiFjSS3Yj1"
   },
   "source": [
    "#### Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2ogF4DB3cvu"
   },
   "outputs": [],
   "source": [
    "class ternaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ternaryClassification,self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) #torch.relu()\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.relu(x)\n",
    "        #x = torch.sigmoid(self.fc3(x))\n",
    "        #x = torch.round(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f2atGU-Jabu"
   },
   "source": [
    "#### Pretrained Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3UGXIzc4ZWC"
   },
   "outputs": [],
   "source": [
    "# split train into train and valid \n",
    "x_puretrain_3_g, x_valid_3_g, y_puretrain_3_g, y_valid_3_g = train_test_split(x_train_3_g,y_train_3_g, test_size=0.2,random_state=2,stratify=y_train_3_g)\n",
    "\n",
    "## train data     \n",
    "train_data = trainData(torch.tensor(x_puretrain_3_g.values), torch.tensor(y_puretrain_3_g.values))\n",
    "\n",
    "## validation data\n",
    "validation_data = trainData(torch.tensor(x_valid_3_g.values), torch.tensor(y_valid_3_g.values))\n",
    "\n",
    "## test data    \n",
    "test_data = testData(torch.tensor(x_test_3_g.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2uyCj5-4ZWL"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_data, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjoG1wo54ZWL"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "t_model = ternaryClassification()\n",
    "#b_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(t_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mfLDx9M3c0v",
    "outputId": "98abeb10-33fc-46fd-9dfe-a2545edf5090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.786512).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.81122 | Validation Loss: 0.78651\n",
      "Validation loss decreased (0.786512 --> 0.780143).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.77864 | Validation Loss: 0.78014\n",
      "Validation loss decreased (0.780143 --> 0.779010).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.77409 | Validation Loss: 0.77901\n",
      "Validation loss decreased (0.779010 --> 0.774775).  Saving model ...\n",
      "Epoch 003: | Train Loss: 0.77079 | Validation Loss: 0.77478\n",
      "Epoch 004: | Train Loss: 0.76807 | Validation Loss: 0.77729\n",
      "Epoch 005: | Train Loss: 0.76508 | Validation Loss: 0.77837\n",
      "Validation loss decreased (0.774775 --> 0.770540).  Saving model ...\n",
      "Epoch 006: | Train Loss: 0.76183 | Validation Loss: 0.77054\n",
      "Validation loss decreased (0.770540 --> 0.766265).  Saving model ...\n",
      "Epoch 007: | Train Loss: 0.75788 | Validation Loss: 0.76626\n",
      "Validation loss decreased (0.766265 --> 0.765228).  Saving model ...\n",
      "Epoch 008: | Train Loss: 0.75484 | Validation Loss: 0.76523\n",
      "Validation loss decreased (0.765228 --> 0.763717).  Saving model ...\n",
      "Epoch 009: | Train Loss: 0.75238 | Validation Loss: 0.76372\n",
      "Validation loss decreased (0.763717 --> 0.760489).  Saving model ...\n",
      "Epoch 010: | Train Loss: 0.75030 | Validation Loss: 0.76049\n",
      "Epoch 011: | Train Loss: 0.74838 | Validation Loss: 0.76162\n",
      "Epoch 012: | Train Loss: 0.74683 | Validation Loss: 0.76117\n",
      "Epoch 013: | Train Loss: 0.74538 | Validation Loss: 0.76136\n",
      "Validation loss decreased (0.760489 --> 0.758962).  Saving model ...\n",
      "Epoch 014: | Train Loss: 0.74424 | Validation Loss: 0.75896\n",
      "Validation loss decreased (0.758962 --> 0.757670).  Saving model ...\n",
      "Epoch 015: | Train Loss: 0.74327 | Validation Loss: 0.75767\n",
      "Validation loss decreased (0.757670 --> 0.756644).  Saving model ...\n",
      "Epoch 016: | Train Loss: 0.74233 | Validation Loss: 0.75664\n",
      "Epoch 017: | Train Loss: 0.74135 | Validation Loss: 0.75834\n",
      "Epoch 018: | Train Loss: 0.74059 | Validation Loss: 0.76066\n",
      "Epoch 019: | Train Loss: 0.73964 | Validation Loss: 0.76127\n",
      "Epoch 020: | Train Loss: 0.73908 | Validation Loss: 0.75733\n",
      "Epoch 021: | Train Loss: 0.73843 | Validation Loss: 0.75670\n",
      "Validation loss decreased (0.756644 --> 0.755759).  Saving model ...\n",
      "Epoch 022: | Train Loss: 0.73768 | Validation Loss: 0.75576\n",
      "Validation loss decreased (0.755759 --> 0.755352).  Saving model ...\n",
      "Epoch 023: | Train Loss: 0.73687 | Validation Loss: 0.75535\n",
      "Epoch 024: | Train Loss: 0.73623 | Validation Loss: 0.75846\n",
      "Epoch 025: | Train Loss: 0.73590 | Validation Loss: 0.75550\n",
      "Epoch 026: | Train Loss: 0.73524 | Validation Loss: 0.75576\n",
      "Epoch 027: | Train Loss: 0.73460 | Validation Loss: 0.75617\n",
      "Epoch 028: | Train Loss: 0.73422 | Validation Loss: 0.75677\n",
      "Epoch 029: | Train Loss: 0.73365 | Validation Loss: 0.75548\n",
      "Epoch 030: | Train Loss: 0.73288 | Validation Loss: 0.75547\n",
      "Epoch 031: | Train Loss: 0.73265 | Validation Loss: 0.75693\n",
      "Epoch 032: | Train Loss: 0.73217 | Validation Loss: 0.75622\n",
      "Epoch 033: | Train Loss: 0.73171 | Validation Loss: 0.75573\n",
      "Validation loss decreased (0.755352 --> 0.754645).  Saving model ...\n",
      "Epoch 034: | Train Loss: 0.73143 | Validation Loss: 0.75465\n",
      "Epoch 035: | Train Loss: 0.73117 | Validation Loss: 0.75502\n",
      "Validation loss decreased (0.754645 --> 0.754097).  Saving model ...\n",
      "Epoch 036: | Train Loss: 0.73063 | Validation Loss: 0.75410\n",
      "Validation loss decreased (0.754097 --> 0.753790).  Saving model ...\n",
      "Epoch 037: | Train Loss: 0.73008 | Validation Loss: 0.75379\n",
      "Epoch 038: | Train Loss: 0.73004 | Validation Loss: 0.75849\n",
      "Epoch 039: | Train Loss: 0.72966 | Validation Loss: 0.75462\n",
      "Epoch 040: | Train Loss: 0.72953 | Validation Loss: 0.75392\n",
      "Epoch 041: | Train Loss: 0.72909 | Validation Loss: 0.75470\n",
      "Validation loss decreased (0.753790 --> 0.753766).  Saving model ...\n",
      "Epoch 042: | Train Loss: 0.72882 | Validation Loss: 0.75377\n",
      "Epoch 043: | Train Loss: 0.72843 | Validation Loss: 0.75381\n",
      "Epoch 044: | Train Loss: 0.72845 | Validation Loss: 0.75468\n",
      "Epoch 045: | Train Loss: 0.72804 | Validation Loss: 0.75405\n",
      "Epoch 046: | Train Loss: 0.72794 | Validation Loss: 0.75439\n",
      "Epoch 047: | Train Loss: 0.72762 | Validation Loss: 0.75381\n",
      "Epoch 048: | Train Loss: 0.72744 | Validation Loss: 0.75467\n",
      "Epoch 049: | Train Loss: 0.72709 | Validation Loss: 0.75386\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "N_EPOCHS = 50\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    t_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      X_batch = X_batch.float()\n",
    "      #X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      y_pred = t_model(X_batch)\n",
    "      loss = criterion(y_pred, y_batch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    t_model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        output = t_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()#*data.size(0)\n",
    "\n",
    "    if (valid_loss/len(valid_loader)) <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss/len(valid_loader)))\n",
    "        torch.save(t_model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss/len(valid_loader)\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_loss/len(train_loader):.5f} | Validation Loss: {valid_loss/len(valid_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7oymlFpDEgT",
    "outputId": "12f4de65-0444-44af-9d5c-37a004094772"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97_RQWXsDEgh"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "from torch import IntTensor\n",
    "def predict3class(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        output = torch.argmax(model(X_batch))\n",
    "        prediction_list.append(IntTensor.item(output))\n",
    "    \n",
    "    return prediction_list#[round(num) for num in prediction_list]\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "  counter = 0\n",
    "  for i in range(0,len(y_true)):\n",
    "    if y_true[i]==y_pred[i]:\n",
    "      counter = counter+1\n",
    "  return counter/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YuVTWswDEgh",
    "outputId": "a590faa4-9464-4400-aebf-106c3b1e8aeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ternary data + Pretrained model, the accuracy for feedforward neural network is 0.6782\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "y_pred = predict3class(t_model, test_loader)  \n",
    "accuracy = get_accuracy(y_test_3_g.to_list(), y_pred)\n",
    "print(\"For Ternary data + Pretrained model, the accuracy for feedforward neural network is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEGnx_q_Jmix"
   },
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO4FG3QoJtYd"
   },
   "outputs": [],
   "source": [
    "# split train into train and valid   \n",
    "x_puretrain_3_my, x_valid_3_my, y_puretrain_3_my, y_valid_3_my = train_test_split(x_train_3_my,y_train_3_my, test_size=0.2,random_state=2,stratify=y_train_3_my)\n",
    "## train data     \n",
    "train_data = trainData(torch.tensor(x_puretrain_3_my.values), torch.tensor(y_puretrain_3_my.values))\n",
    "\n",
    "## validation data\n",
    "validation_data = trainData(torch.tensor(x_valid_3_my.values), torch.tensor(y_valid_3_my.values))\n",
    "\n",
    "## test data    \n",
    "test_data = testData(torch.tensor(x_test_3_my.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5u78jQi5JtYe"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_data, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFuFmGB9JtYe"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "t_model = ternaryClassification()\n",
    "#b_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(t_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3FBnhRETJpQh",
    "outputId": "266e68c6-e732-40b2-af6c-a54458900283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.734760).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.75256 | Validation Loss: 0.73476\n",
      "Validation loss decreased (0.734760 --> 0.730281).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.72555 | Validation Loss: 0.73028\n",
      "Validation loss decreased (0.730281 --> 0.720353).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.71804 | Validation Loss: 0.72035\n",
      "Validation loss decreased (0.720353 --> 0.717478).  Saving model ...\n",
      "Epoch 003: | Train Loss: 0.71215 | Validation Loss: 0.71748\n",
      "Validation loss decreased (0.717478 --> 0.715590).  Saving model ...\n",
      "Epoch 004: | Train Loss: 0.70773 | Validation Loss: 0.71559\n",
      "Validation loss decreased (0.715590 --> 0.714481).  Saving model ...\n",
      "Epoch 005: | Train Loss: 0.70436 | Validation Loss: 0.71448\n",
      "Validation loss decreased (0.714481 --> 0.712818).  Saving model ...\n",
      "Epoch 006: | Train Loss: 0.70195 | Validation Loss: 0.71282\n",
      "Epoch 007: | Train Loss: 0.69960 | Validation Loss: 0.71536\n",
      "Validation loss decreased (0.712818 --> 0.710890).  Saving model ...\n",
      "Epoch 008: | Train Loss: 0.69801 | Validation Loss: 0.71089\n",
      "Epoch 009: | Train Loss: 0.69639 | Validation Loss: 0.71109\n",
      "Validation loss decreased (0.710890 --> 0.710839).  Saving model ...\n",
      "Epoch 010: | Train Loss: 0.69495 | Validation Loss: 0.71084\n",
      "Epoch 011: | Train Loss: 0.69347 | Validation Loss: 0.71385\n",
      "Validation loss decreased (0.710839 --> 0.710210).  Saving model ...\n",
      "Epoch 012: | Train Loss: 0.69285 | Validation Loss: 0.71021\n",
      "Validation loss decreased (0.710210 --> 0.709644).  Saving model ...\n",
      "Epoch 013: | Train Loss: 0.69185 | Validation Loss: 0.70964\n",
      "Epoch 014: | Train Loss: 0.69114 | Validation Loss: 0.71206\n",
      "Epoch 015: | Train Loss: 0.69058 | Validation Loss: 0.71372\n",
      "Epoch 016: | Train Loss: 0.69017 | Validation Loss: 0.71075\n",
      "Epoch 017: | Train Loss: 0.68940 | Validation Loss: 0.71031\n",
      "Epoch 018: | Train Loss: 0.68878 | Validation Loss: 0.71093\n",
      "Epoch 019: | Train Loss: 0.68822 | Validation Loss: 0.71258\n",
      "Epoch 020: | Train Loss: 0.68787 | Validation Loss: 0.71183\n",
      "Epoch 021: | Train Loss: 0.68736 | Validation Loss: 0.71405\n",
      "Epoch 022: | Train Loss: 0.68753 | Validation Loss: 0.71283\n",
      "Epoch 023: | Train Loss: 0.68686 | Validation Loss: 0.71040\n",
      "Epoch 024: | Train Loss: 0.68618 | Validation Loss: 0.71218\n",
      "Epoch 025: | Train Loss: 0.68611 | Validation Loss: 0.71119\n",
      "Epoch 026: | Train Loss: 0.68610 | Validation Loss: 0.71101\n",
      "Epoch 027: | Train Loss: 0.68542 | Validation Loss: 0.71044\n",
      "Epoch 028: | Train Loss: 0.68538 | Validation Loss: 0.71189\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "N_EPOCHS = 30\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    t_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      X_batch = X_batch.float()\n",
    "      #X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      y_pred = t_model(X_batch)\n",
    "      loss = criterion(y_pred, y_batch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    t_model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        output = t_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()#*data.size(0)\n",
    "\n",
    "    if (valid_loss/len(valid_loader)) <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss/len(valid_loader)))\n",
    "        torch.save(t_model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss/len(valid_loader)\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_loss/len(train_loader):.5f} | Validation Loss: {valid_loss/len(valid_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYrgq2LbJ1lF"
   },
   "outputs": [],
   "source": [
    "t_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPBI9gc1J1lF",
    "outputId": "68c7d273-219a-4d00-a5a5-7f52b5ff4281"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6628"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "y_pred = predict3class(t_model, test_loader)  \n",
    "accuracy = get_accuracy(y_test_3_my.to_list(), y_pred)\n",
    "print(\"For Ternary data + My model, the accuracy for feedforward neural network is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTMr6ooqQt1D"
   },
   "source": [
    "#### part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-RDR0fVOrFY"
   },
   "source": [
    "#### create f10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFTaGHmLQwBi"
   },
   "outputs": [],
   "source": [
    "def compute_avg(s,input_model):\n",
    "  words = s.split()\n",
    "  num_of_words = len(words)\n",
    "  if num_of_words==0:\n",
    "    return [0]*300\n",
    "  else:\n",
    "    counter = 0\n",
    "    current_total_vector = [0]*300\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        word_vector = input_model[each_word]\n",
    "        current_total_vector = [a+b for a,b in zip(current_total_vector,word_vector)]\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter >=10:\n",
    "        break\n",
    "    if counter>0:\n",
    "      avg_vector = [x / counter for x in current_total_vector]\n",
    "    else:\n",
    "      avg_vector = [0]*300\n",
    "    return avg_vector\n",
    "\n",
    "def get_avg_W2V(input_model1,df1):\n",
    "  temp_df = df1[['review_body']].copy()\n",
    "  temp_df['all_vec'] = temp_df.apply(lambda y: compute_avg(y['review_body'],input_model1),axis = 1)\n",
    "  vec_df = temp_df['all_vec'].apply(pd.Series)\n",
    "  return vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCAGzxWTL9Gf"
   },
   "outputs": [],
   "source": [
    "# For ternary data, compute the average for the first 10 words for both Word2Vec models\n",
    "from sklearn.model_selection import train_test_split\n",
    "ternary_label = clean_data['label']\n",
    "\n",
    "# my model\n",
    "my_vec_df_ternary_f10 = get_avg_W2V(mymodel.wv,clean_data)\n",
    "x_train_3_my_f10, x_test_3_my_f10, y_train_3_my_f10, y_test_3_my_f10 = train_test_split(my_vec_df_ternary_f10,ternary_label, test_size=0.2,random_state=2,stratify=ternary_label)\n",
    "\n",
    "# google model\n",
    "g_vec_df_ternary_f10 = get_avg_W2V(wv_g,clean_data)\n",
    "x_train_3_g_f10, x_test_3_g_f10, y_train_3_g_f10, y_test_3_g_f10 = train_test_split(g_vec_df_ternary_f10,ternary_label, test_size=0.2,random_state=2,stratify=ternary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQy9O9lZMCOg"
   },
   "outputs": [],
   "source": [
    "#--------------------binary data-------------------------- \n",
    "# my model\n",
    "my_data_df_ternary_f10 = my_vec_df_ternary_f10.copy()\n",
    "my_data_df_ternary_f10['label'] = ternary_label\n",
    "my_data_df_binary_f10 = my_data_df_ternary_f10[my_data_df_ternary_f10['label']!=1]\n",
    "my_vec_df_binary_f10 = my_data_df_binary_f10.iloc[:, :-1].values\n",
    "my_binary_label_f10 = my_data_df_binary_f10.iloc[:, 300].values\n",
    "my_binary_label_f10 = np.where(my_binary_label_f10 == 2, 1, my_binary_label_f10)\n",
    "x_train_2_my_f10, x_test_2_my_f10, y_train_2_my_f10, y_test_2_my_f10 = train_test_split(my_vec_df_binary_f10,my_binary_label_f10, test_size=0.2,random_state=2,stratify=my_binary_label_f10)\n",
    "\n",
    "# google model\n",
    "g_data_df_ternary_f10 = g_vec_df_ternary_f10.copy()\n",
    "g_data_df_ternary_f10['label'] = ternary_label\n",
    "g_data_df_binary_f10 = g_data_df_ternary_f10[g_data_df_ternary_f10['label']!=1]\n",
    "g_vec_df_binary_f10 = g_data_df_binary_f10.iloc[:, :-1].values\n",
    "g_binary_label_f10 = g_data_df_binary_f10.iloc[:, 300].values\n",
    "g_binary_label_f10 = np.where(g_binary_label_f10 == 2, 1, g_binary_label_f10)\n",
    "x_train_2_g_f10, x_test_2_g_f10, y_train_2_g_f10, y_test_2_g_f10 = train_test_split(g_vec_df_binary_f10,g_binary_label_f10, test_size=0.2,random_state=2,stratify=g_binary_label_f10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9XbWoLFOuqj"
   },
   "source": [
    "#### binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE1Func8O916"
   },
   "source": [
    "#### Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LxjrqscPAzP"
   },
   "outputs": [],
   "source": [
    "# split train into train and valid \n",
    "x_puretrain_2_g_f10, x_valid_2_g_f10, y_puretrain_2_g_f10, y_valid_2_g_f10 = train_test_split(x_train_2_g,y_train_2_g_f10, test_size=0.2,random_state=2,stratify=y_train_2_g_f10)\n",
    "\n",
    "## train data     \n",
    "train_data = trainData(torch.tensor(x_puretrain_2_g_f10), torch.tensor(y_puretrain_2_g_f10))\n",
    "\n",
    "## validation data\n",
    "validation_data = trainData(torch.tensor(x_valid_2_g_f10), torch.tensor(y_valid_2_g_f10))\n",
    "\n",
    "## test data    \n",
    "test_data = testData(torch.tensor(x_test_2_g_f10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQrBLnj-PAzP"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_data, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQTKV500PAzQ"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "b_model = binaryClassification()\n",
    "#b_model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(b_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uB839TK2PAzR",
    "outputId": "f88804e8-8a1c-4671-9a40-37f5602826dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.449485).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.55349 | Validation Loss: 0.44949\n",
      "Validation loss decreased (0.449485 --> 0.420397).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.43723 | Validation Loss: 0.42040\n",
      "Validation loss decreased (0.420397 --> 0.417392).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.41975 | Validation Loss: 0.41739\n",
      "Validation loss decreased (0.417392 --> 0.408047).  Saving model ...\n",
      "Epoch 003: | Train Loss: 0.41063 | Validation Loss: 0.40805\n",
      "Validation loss decreased (0.408047 --> 0.406333).  Saving model ...\n",
      "Epoch 004: | Train Loss: 0.40370 | Validation Loss: 0.40633\n",
      "Validation loss decreased (0.406333 --> 0.404096).  Saving model ...\n",
      "Epoch 005: | Train Loss: 0.39799 | Validation Loss: 0.40410\n",
      "Validation loss decreased (0.404096 --> 0.401656).  Saving model ...\n",
      "Epoch 006: | Train Loss: 0.39356 | Validation Loss: 0.40166\n",
      "Epoch 007: | Train Loss: 0.38993 | Validation Loss: 0.40220\n",
      "Epoch 008: | Train Loss: 0.38556 | Validation Loss: 0.40205\n",
      "Validation loss decreased (0.401656 --> 0.400085).  Saving model ...\n",
      "Epoch 009: | Train Loss: 0.38353 | Validation Loss: 0.40009\n",
      "Validation loss decreased (0.400085 --> 0.399309).  Saving model ...\n",
      "Epoch 010: | Train Loss: 0.37892 | Validation Loss: 0.39931\n",
      "Validation loss decreased (0.399309 --> 0.395422).  Saving model ...\n",
      "Epoch 011: | Train Loss: 0.37762 | Validation Loss: 0.39542\n",
      "Epoch 012: | Train Loss: 0.37268 | Validation Loss: 0.40029\n",
      "Epoch 013: | Train Loss: 0.37041 | Validation Loss: 0.39752\n",
      "Epoch 014: | Train Loss: 0.36705 | Validation Loss: 0.39989\n",
      "Validation loss decreased (0.395422 --> 0.393024).  Saving model ...\n",
      "Epoch 015: | Train Loss: 0.36271 | Validation Loss: 0.39302\n",
      "Epoch 016: | Train Loss: 0.36013 | Validation Loss: 0.39868\n",
      "Epoch 017: | Train Loss: 0.35666 | Validation Loss: 0.40077\n",
      "Epoch 018: | Train Loss: 0.35281 | Validation Loss: 0.39594\n",
      "Epoch 019: | Train Loss: 0.34882 | Validation Loss: 0.39542\n",
      "Epoch 020: | Train Loss: 0.34477 | Validation Loss: 0.39525\n",
      "Epoch 021: | Train Loss: 0.34308 | Validation Loss: 0.39964\n",
      "Epoch 022: | Train Loss: 0.33828 | Validation Loss: 0.39789\n",
      "Epoch 023: | Train Loss: 0.33366 | Validation Loss: 0.39562\n",
      "Epoch 024: | Train Loss: 0.33163 | Validation Loss: 0.39467\n",
      "Epoch 025: | Train Loss: 0.32634 | Validation Loss: 0.40087\n",
      "Validation loss decreased (0.393024 --> 0.392561).  Saving model ...\n",
      "Epoch 026: | Train Loss: 0.32302 | Validation Loss: 0.39256\n",
      "Epoch 027: | Train Loss: 0.31931 | Validation Loss: 0.39367\n",
      "Epoch 028: | Train Loss: 0.31530 | Validation Loss: 0.39788\n",
      "Epoch 029: | Train Loss: 0.31039 | Validation Loss: 0.39705\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "N_EPOCHS = 30\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    b_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      X_batch = X_batch.float()\n",
    "      #X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      y_pred = b_model(X_batch)\n",
    "      loss = criterion(y_pred.flatten(), y_batch.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    b_model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        output = b_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.flatten(), target.float())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    if (valid_loss/len(valid_loader)) <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss/len(valid_loader)))\n",
    "        torch.save(b_model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss/len(valid_loader)\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_loss/len(train_loader):.5f} | Validation Loss: {valid_loss/len(valid_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1AtmWwvJPAzR",
    "outputId": "2c73e8a9-1d68-44b3-d9ba-1b072a9e2b2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2gNnTHdPAzR"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        output = model(X_batch)\n",
    "        prediction_list.append(output.item())\n",
    "    \n",
    "    return [round(num) for num in prediction_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5hKeokNPAzR",
    "outputId": "8a985344-9be7-40ff-de34-e3f2622df142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Binary data + Pretrained model + first 10 vectors, the accuracy for feedforward neural network is 0.764\n",
      "Other useful values are shown below:\n",
      "The precision, recall and f1-score of testing dataset are 0.7595870206489675, 0.7725, 0.7659890927119484.\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "y_pred = predict(b_model, test_loader)  \n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2_g_f10, np.array(y_pred)).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "print(\"For Binary data + Pretrained model + first 10 vectors, the accuracy for feedforward neural network is \"+str(accuracy))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Other useful values are shown below:\")\n",
    "answer_str = 'The precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcFQs8UaP7Gs"
   },
   "source": [
    "#### My model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G42S1QeOQKor"
   },
   "outputs": [],
   "source": [
    "# split train into train and valid\n",
    "x_puretrain_2_my_f10, x_valid_2_my_f10, y_puretrain_2_my_f10, y_valid_2_my_f10 = train_test_split(x_train_2_my_f10,y_train_2_my_f10, test_size=0.2,random_state=2,stratify=y_train_2_my_f10)\n",
    "\n",
    "## train data     \n",
    "train_data = trainData(torch.tensor(x_puretrain_2_my_f10), torch.tensor(y_puretrain_2_my_f10))\n",
    "\n",
    "## validation data\n",
    "validation_data = trainData(torch.tensor(x_valid_2_my_f10), torch.tensor(y_valid_2_my_f10))\n",
    "\n",
    "## test data    \n",
    "test_data = testData(torch.tensor(x_test_2_my_f10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7o_jVTPQKor"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_data, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isMvacTIQKos"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "b_model = binaryClassification()\n",
    "#b_model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(b_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mxiKfd6QKos",
    "outputId": "7b35f34a-d92a-4cf6-e546-a8b4ca218350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.485828).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.53516 | Validation Loss: 0.48583\n",
      "Validation loss decreased (0.485828 --> 0.477947).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.47854 | Validation Loss: 0.47795\n",
      "Validation loss decreased (0.477947 --> 0.475256).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.46655 | Validation Loss: 0.47526\n",
      "Validation loss decreased (0.475256 --> 0.473613).  Saving model ...\n",
      "Epoch 003: | Train Loss: 0.45866 | Validation Loss: 0.47361\n",
      "Validation loss decreased (0.473613 --> 0.470081).  Saving model ...\n",
      "Epoch 004: | Train Loss: 0.45048 | Validation Loss: 0.47008\n",
      "Epoch 005: | Train Loss: 0.44150 | Validation Loss: 0.47333\n",
      "Epoch 006: | Train Loss: 0.43419 | Validation Loss: 0.47038\n",
      "Epoch 007: | Train Loss: 0.42693 | Validation Loss: 0.47148\n",
      "Epoch 008: | Train Loss: 0.41784 | Validation Loss: 0.47633\n",
      "Epoch 009: | Train Loss: 0.41136 | Validation Loss: 0.47656\n",
      "Epoch 010: | Train Loss: 0.40389 | Validation Loss: 0.48298\n",
      "Epoch 011: | Train Loss: 0.39645 | Validation Loss: 0.47988\n",
      "Epoch 012: | Train Loss: 0.38976 | Validation Loss: 0.49113\n",
      "Epoch 013: | Train Loss: 0.38437 | Validation Loss: 0.48720\n",
      "Epoch 014: | Train Loss: 0.37747 | Validation Loss: 0.49875\n",
      "Epoch 015: | Train Loss: 0.37024 | Validation Loss: 0.49939\n",
      "Epoch 016: | Train Loss: 0.36374 | Validation Loss: 0.50367\n",
      "Epoch 017: | Train Loss: 0.35634 | Validation Loss: 0.51230\n",
      "Epoch 018: | Train Loss: 0.34947 | Validation Loss: 0.51590\n",
      "Epoch 019: | Train Loss: 0.34350 | Validation Loss: 0.52739\n",
      "Epoch 020: | Train Loss: 0.33807 | Validation Loss: 0.52889\n",
      "Epoch 021: | Train Loss: 0.33291 | Validation Loss: 0.53171\n",
      "Epoch 022: | Train Loss: 0.32626 | Validation Loss: 0.54067\n",
      "Epoch 023: | Train Loss: 0.32178 | Validation Loss: 0.54708\n",
      "Epoch 024: | Train Loss: 0.31520 | Validation Loss: 0.55469\n",
      "Epoch 025: | Train Loss: 0.30952 | Validation Loss: 0.56639\n",
      "Epoch 026: | Train Loss: 0.30340 | Validation Loss: 0.56651\n",
      "Epoch 027: | Train Loss: 0.29832 | Validation Loss: 0.58208\n",
      "Epoch 028: | Train Loss: 0.29094 | Validation Loss: 0.57578\n",
      "Epoch 029: | Train Loss: 0.28838 | Validation Loss: 0.57571\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "N_EPOCHS = 30\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    b_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      X_batch = X_batch.float()\n",
    "      #X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      y_pred = b_model(X_batch)\n",
    "      loss = criterion(y_pred.flatten(), y_batch.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    b_model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        output = b_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.flatten(), target.float())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    if (valid_loss/len(valid_loader)) <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss/len(valid_loader)))\n",
    "        torch.save(b_model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss/len(valid_loader)\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_loss/len(train_loader):.5f} | Validation Loss: {valid_loss/len(valid_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ofk9TY0bQKos",
    "outputId": "97556b7a-1615-4406-84c2-56716d8e42b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "By9KIwVqQKos"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        output = model(X_batch)\n",
    "        prediction_list.append(output.item())\n",
    "    \n",
    "    return [round(num) for num in prediction_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vn8xqDwXQKot",
    "outputId": "284c83c3-2345-448c-fb9d-99251fbd7bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Binary data + My model + first 10 vectors, the accuracy for feedforward neural network is 0.77575\n",
      "Other useful values are shown below:\n",
      "The precision, recall and f1-score of testing dataset are 0.7767185148018063, 0.774, 0.7753568745304283.\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "y_pred = predict(b_model, test_loader)  \n",
    "tn, fp, fn, tp = confusion_matrix(y_test_2_my_f10, np.array(y_pred)).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "print(\"For Binary data + My model + first 10 vectors, the accuracy for feedforward neural network is \"+str(accuracy))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Other useful values are shown below:\")\n",
    "answer_str = 'The precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUcxAuVNQmEe"
   },
   "source": [
    "#### Ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt9t7p4wQpmb"
   },
   "source": [
    "#### Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98zNnTk-Q5Tu"
   },
   "outputs": [],
   "source": [
    "# split train into train and valid   \n",
    "x_puretrain_3_g_f10, x_valid_3_g_f10, y_puretrain_3_g_f10, y_valid_3_g_f10 = train_test_split(x_train_3_g_f10,y_train_3_g_f10, test_size=0.2,random_state=2,stratify=y_train_3_g_f10)\n",
    "\n",
    "## train data     \n",
    "train_data = trainData(torch.tensor(x_puretrain_3_g_f10.values), torch.tensor(y_puretrain_3_g_f10.values))\n",
    "\n",
    "## validation data\n",
    "validation_data = trainData(torch.tensor(x_valid_3_g_f10.values), torch.tensor(y_valid_3_g_f10.values))\n",
    "\n",
    "## test data    \n",
    "test_data = testData(torch.tensor(x_test_3_g_f10.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmnmIZBvQ5Tv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_data, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oT22bwyYQ5Tv"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "t_model = ternaryClassification()\n",
    "#b_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(t_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueJ7MJNgQ5Tv",
    "outputId": "d9b91c8c-0dd0-43ce-8ec0-2d8a0e814fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.993760).  Saving model ...\n",
      "Epoch 000: | Train Loss: 1.01531 | Validation Loss: 0.99376\n",
      "Validation loss decreased (0.993760 --> 0.991653).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.98476 | Validation Loss: 0.99165\n",
      "Validation loss decreased (0.991653 --> 0.990568).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.97987 | Validation Loss: 0.99057\n",
      "Epoch 003: | Train Loss: 0.97741 | Validation Loss: 0.99184\n",
      "Epoch 004: | Train Loss: 0.97572 | Validation Loss: 0.99077\n",
      "Epoch 005: | Train Loss: 0.97346 | Validation Loss: 0.99127\n",
      "Epoch 006: | Train Loss: 0.97240 | Validation Loss: 0.99186\n",
      "Epoch 007: | Train Loss: 0.97112 | Validation Loss: 0.99253\n",
      "Epoch 008: | Train Loss: 0.97020 | Validation Loss: 0.99478\n",
      "Epoch 009: | Train Loss: 0.96822 | Validation Loss: 0.99236\n",
      "Epoch 010: | Train Loss: 0.96818 | Validation Loss: 0.99385\n",
      "Epoch 011: | Train Loss: 0.96797 | Validation Loss: 0.99252\n",
      "Epoch 012: | Train Loss: 0.96679 | Validation Loss: 0.99524\n",
      "Epoch 013: | Train Loss: 0.96535 | Validation Loss: 0.99519\n",
      "Epoch 014: | Train Loss: 0.96527 | Validation Loss: 0.99359\n",
      "Epoch 015: | Train Loss: 0.96453 | Validation Loss: 0.99421\n",
      "Epoch 016: | Train Loss: 0.96361 | Validation Loss: 0.99913\n",
      "Epoch 017: | Train Loss: 0.96313 | Validation Loss: 0.99511\n",
      "Epoch 018: | Train Loss: 0.96225 | Validation Loss: 0.99676\n",
      "Epoch 019: | Train Loss: 0.96107 | Validation Loss: 0.99485\n",
      "Epoch 020: | Train Loss: 0.96105 | Validation Loss: 0.99562\n",
      "Epoch 021: | Train Loss: 0.96086 | Validation Loss: 0.99542\n",
      "Epoch 022: | Train Loss: 0.95936 | Validation Loss: 0.99671\n",
      "Epoch 023: | Train Loss: 0.95818 | Validation Loss: 0.99983\n",
      "Epoch 024: | Train Loss: 0.95809 | Validation Loss: 0.99699\n",
      "Epoch 025: | Train Loss: 0.95719 | Validation Loss: 0.99801\n",
      "Epoch 026: | Train Loss: 0.95549 | Validation Loss: 1.00107\n",
      "Epoch 027: | Train Loss: 0.95570 | Validation Loss: 0.99760\n",
      "Epoch 028: | Train Loss: 0.95434 | Validation Loss: 0.99950\n",
      "Epoch 029: | Train Loss: 0.95416 | Validation Loss: 1.00227\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "N_EPOCHS = 30\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    t_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      X_batch = X_batch.float()\n",
    "      #X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      y_pred = t_model(X_batch)\n",
    "      loss = criterion(y_pred, y_batch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    t_model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        output = t_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    if (valid_loss/len(valid_loader)) <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss/len(valid_loader)))\n",
    "        torch.save(t_model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss/len(valid_loader)\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_loss/len(train_loader):.5f} | Validation Loss: {valid_loss/len(valid_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMHWOHWrQ5Tv",
    "outputId": "06f11fce-417c-439d-84fd-be86dcc13227"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnllVzYTQ5Tw",
    "outputId": "419d7302-1c1c-46b9-968a-c093cb5ac11d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ternary data + Pretrained model + first 10 vectors, the accuracy for feedforward neural network is 0.5988\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "y_pred = predict3class(t_model, test_loader)  \n",
    "accuracy = get_accuracy(y_test_3_g_f10.to_list(), y_pred)\n",
    "print(\"For Ternary data + Pretrained model + first 10 vectors, the accuracy for feedforward neural network is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AxQvwosQsvb"
   },
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJq_rVg2RnKN"
   },
   "outputs": [],
   "source": [
    "# split train into train and valid \n",
    "x_puretrain_3_my_f10, x_valid_3_my_f10, y_puretrain_3_my_f10, y_valid_3_my_f10 = train_test_split(x_train_3_my_f10,y_train_3_my_f10, test_size=0.2,random_state=2,stratify=y_train_3_my_f10)\n",
    "## train data     \n",
    "train_data = trainData(torch.tensor(x_puretrain_3_my_f10.values), torch.tensor(y_puretrain_3_my_f10.values))\n",
    "\n",
    "## validation data\n",
    "validation_data = trainData(torch.tensor(x_valid_3_my_f10.values), torch.tensor(y_valid_3_my_f10.values))\n",
    "\n",
    "## test data    \n",
    "test_data = testData(torch.tensor(x_test_3_my_f10.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oXQsrfGRnKO"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=validation_data, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q5fasgERnKO"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "t_model = ternaryClassification()\n",
    "#b_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(t_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_HBmcKlRnKO",
    "outputId": "9c48733c-b9d8-484e-9f8f-7001d9303012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.871216).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.92821 | Validation Loss: 0.87122\n",
      "Validation loss decreased (0.871216 --> 0.863455).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.86635 | Validation Loss: 0.86345\n",
      "Epoch 002: | Train Loss: 0.85840 | Validation Loss: 0.86355\n",
      "Validation loss decreased (0.863455 --> 0.853500).  Saving model ...\n",
      "Epoch 003: | Train Loss: 0.85217 | Validation Loss: 0.85350\n",
      "Validation loss decreased (0.853500 --> 0.851163).  Saving model ...\n",
      "Epoch 004: | Train Loss: 0.84575 | Validation Loss: 0.85116\n",
      "Epoch 005: | Train Loss: 0.84172 | Validation Loss: 0.85217\n",
      "Validation loss decreased (0.851163 --> 0.849143).  Saving model ...\n",
      "Epoch 006: | Train Loss: 0.83733 | Validation Loss: 0.84914\n",
      "Epoch 007: | Train Loss: 0.83465 | Validation Loss: 0.85494\n",
      "Epoch 008: | Train Loss: 0.83006 | Validation Loss: 0.85059\n",
      "Epoch 009: | Train Loss: 0.82777 | Validation Loss: 0.85142\n",
      "Epoch 010: | Train Loss: 0.82424 | Validation Loss: 0.85637\n",
      "Epoch 011: | Train Loss: 0.82126 | Validation Loss: 0.85375\n",
      "Epoch 012: | Train Loss: 0.81805 | Validation Loss: 0.85481\n",
      "Epoch 013: | Train Loss: 0.81581 | Validation Loss: 0.85870\n",
      "Epoch 014: | Train Loss: 0.81356 | Validation Loss: 0.85522\n",
      "Epoch 015: | Train Loss: 0.81210 | Validation Loss: 0.86236\n",
      "Epoch 016: | Train Loss: 0.81009 | Validation Loss: 0.86062\n",
      "Epoch 017: | Train Loss: 0.80827 | Validation Loss: 0.85868\n",
      "Epoch 018: | Train Loss: 0.80643 | Validation Loss: 0.86773\n",
      "Epoch 019: | Train Loss: 0.80467 | Validation Loss: 0.86629\n",
      "Epoch 020: | Train Loss: 0.80325 | Validation Loss: 0.85942\n",
      "Epoch 021: | Train Loss: 0.80245 | Validation Loss: 0.86377\n",
      "Epoch 022: | Train Loss: 0.80006 | Validation Loss: 0.86642\n",
      "Epoch 023: | Train Loss: 0.79917 | Validation Loss: 0.86287\n",
      "Epoch 024: | Train Loss: 0.79764 | Validation Loss: 0.86558\n",
      "Epoch 025: | Train Loss: 0.79525 | Validation Loss: 0.86659\n",
      "Epoch 026: | Train Loss: 0.79571 | Validation Loss: 0.86521\n",
      "Epoch 027: | Train Loss: 0.79384 | Validation Loss: 0.87000\n",
      "Epoch 028: | Train Loss: 0.79347 | Validation Loss: 0.86667\n",
      "Epoch 029: | Train Loss: 0.79182 | Validation Loss: 0.86935\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "N_EPOCHS = 30\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    t_model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "      X_batch = X_batch.float()\n",
    "      #X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      y_pred = t_model(X_batch)\n",
    "      loss = criterion(y_pred, y_batch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    t_model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data = data.float()\n",
    "        output = t_model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()#*data.size(0)\n",
    "\n",
    "    if (valid_loss/len(valid_loader)) <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss/len(valid_loader)))\n",
    "        torch.save(t_model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss/len(valid_loader)\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_loss/len(train_loader):.5f} | Validation Loss: {valid_loss/len(valid_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERSVSl3IRnKP",
    "outputId": "510ce023-f89e-4bfb-9cd8-22833a8882d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpufyj5ERnKP",
    "outputId": "f0252073-1758-4b5b-b40c-9914a45deb51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ternary data + My model + first 10 vectors, the accuracy for feedforward neural network is 0.6196\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "y_pred = predict3class(t_model, test_loader)  \n",
    "accuracy = get_accuracy(y_test_3_my_f10.to_list(), y_pred)\n",
    "print(\"For Ternary data + My model + first 10 vectors, the accuracy for feedforward neural network is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkZmmZ6KQylr"
   },
   "source": [
    "## 5. Recurrent Neutral Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0sIzSYDQy0w"
   },
   "source": [
    "#### part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpX34gJ7Fpq6"
   },
   "source": [
    "#### Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH1K-GlbzVDz"
   },
   "source": [
    "#### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjze25I9ztNR"
   },
   "outputs": [],
   "source": [
    "def convert_label(l):\n",
    "  if l==2:\n",
    "    return 1;\n",
    "  else:\n",
    "    return l;\n",
    "binary_data['label']=binary_data['label'].apply(convert_label)\n",
    "binary_train, binary_test = train_test_split(binary_data, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoK1jyHHAO6_"
   },
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FFiJud8VqI4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        #output = self.softmax(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2rfe4sSrrhj"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "rnn = RNN(300, n_hidden, 1)\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.BCELoss()#CrossEntropyLoss() #nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHUytqwhecla"
   },
   "outputs": [],
   "source": [
    "def train(y_true, review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  optimizer.zero_grad()\n",
    "  words = review.split()\n",
    "  counter = 0\n",
    "  for each_word in words:\n",
    "    try:\n",
    "      input = torch.tensor([current_model[each_word]])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "      counter = counter+1\n",
    "    except:\n",
    "      pass\n",
    "    if counter==50:\n",
    "      break\n",
    "  \n",
    "  if counter<50:\n",
    "    for i in range(counter,50):\n",
    "      input = torch.tensor([[0]*300])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  loss = criterion(output.flatten(), torch.tensor([y_true]).float())\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def rnn_test(review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  with torch.no_grad():\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        input = torch.tensor([current_model[each_word]])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter==50:\n",
    "        break\n",
    "    \n",
    "    if counter<50:\n",
    "      for i in range(counter,50):\n",
    "        input = torch.tensor([[0]*300])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMX6eCvztjEE"
   },
   "outputs": [],
   "source": [
    "num_train = binary_train.iloc[:,0].size\n",
    "current_loss = 0\n",
    "for i in range(0,num_train):\n",
    "  sentence = binary_train.iloc[i].loc['review_body']\n",
    "  label = binary_train.iloc[i].loc['label']\n",
    "  output, loss = train(label, sentence, mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yRmWfn8vXSA",
    "outputId": "0cc57b94-070e-48bd-963d-f98a5fb552ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Binary data + My model, the accuracy for recurrent neural network is 0.5295\n",
      "Other useful values are shown below:\n",
      "The precision, recall and f1-score of testing dataset are 0.5204532891100055, 0.9275862068965517, 0.6667847025495751.\n"
     ]
    }
   ],
   "source": [
    "num_test = binary_test.iloc[:,0].size\n",
    "output_list = []\n",
    "for i in range(0,num_test):\n",
    "  sentence = binary_test.iloc[i].loc['review_body']\n",
    "  label = binary_test.iloc[i].loc['label']\n",
    "  output = rnn_test(sentence, mymodel)\n",
    "  output = round(output.item())\n",
    "  output_list.append(output)\n",
    "  \n",
    "tn, fp, fn, tp = confusion_matrix(binary_test['label'], output_list).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "print(\"For Binary data + My model, the accuracy for recurrent neural network is \"+str(accuracy))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Other useful values are shown below:\")\n",
    "answer_str = 'The precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56vsstN-ATA_"
   },
   "source": [
    "#### Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GL0xbgs9C-ve"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        #output = self.softmax(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thtFmSAXCDpT"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "rnn = RNN(300, n_hidden, 1)\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.BCELoss()#CrossEntropyLoss() #nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcL4jyq8DFuh"
   },
   "outputs": [],
   "source": [
    "def train(y_true, review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  optimizer.zero_grad()\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  for each_word in words:\n",
    "    try:\n",
    "      input = torch.tensor([current_model[each_word]])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "      counter = counter+1\n",
    "    except:\n",
    "      pass\n",
    "    if counter==50:\n",
    "      break\n",
    "  \n",
    "  if counter<50:\n",
    "    for i in range(counter,50):\n",
    "      input = torch.tensor([[0]*300])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  loss = criterion(output.flatten(), torch.tensor([y_true]).float())\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def rnn_test(review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  with torch.no_grad():\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        input = torch.tensor([current_model[each_word]])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter==50:\n",
    "        break\n",
    "    \n",
    "    if counter<50:\n",
    "      for i in range(counter,50):\n",
    "        input = torch.tensor([[0]*300])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33GgY43pCDpU"
   },
   "outputs": [],
   "source": [
    "num_train = binary_train.iloc[:,0].size\n",
    "current_loss = 0\n",
    "for i in range(0,num_train):\n",
    "  sentence = binary_train.iloc[i].loc['review_body']\n",
    "  label = binary_train.iloc[i].loc['label']\n",
    "  output, loss = train(label, sentence, wv_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlnM8D0bCDpU",
    "outputId": "5e2a1ef1-d015-408c-864d-1e863f1244ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Binary data + Pretrained model, the accuracy for recurrent neural network is 0.52775\n",
      "Other useful values are shown below:\n",
      "The precision, recall and f1-score of testing dataset are 0.5203816131830009, 0.8866995073891626, 0.655857168883221.\n"
     ]
    }
   ],
   "source": [
    "num_test = binary_test.iloc[:,0].size\n",
    "output_list = []\n",
    "for i in range(0,num_test):\n",
    "  sentence = binary_test.iloc[i].loc['review_body']\n",
    "  label = binary_test.iloc[i].loc['label']\n",
    "  output = rnn_test(sentence, wv_g)\n",
    "  output = round(output.item())\n",
    "  output_list.append(output)\n",
    "\n",
    "  \n",
    "tn, fp, fn, tp = confusion_matrix(binary_test['label'], output_list).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "print(\"For Binary data + Pretrained model, the accuracy for recurrent neural network is \"+str(accuracy))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Other useful values are shown below:\")\n",
    "answer_str = 'The precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSI9X1H3DVdP"
   },
   "source": [
    "#### ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gySk6gnWGqxm"
   },
   "source": [
    "#### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOkPyAyRGqxn"
   },
   "outputs": [],
   "source": [
    "ternary_train, ternary_test = train_test_split(clean_data, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-AZnCarDgv3"
   },
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTygokdMDfwu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        #output = torch.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpxaBL5cDfwu"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "rnn = RNN(300, n_hidden, 3)\n",
    "LEARNING_RATE = 0.002\n",
    "criterion = nn.CrossEntropyLoss() #nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsjW8NbGDfwu"
   },
   "outputs": [],
   "source": [
    "def train(y_true, review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  optimizer.zero_grad()\n",
    "  words = review.split()\n",
    "  counter = 0\n",
    "  for each_word in words:\n",
    "    try:\n",
    "      input = torch.tensor([current_model[each_word]])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "      counter = counter+1\n",
    "    except:\n",
    "      pass\n",
    "    if counter==50:\n",
    "      break\n",
    "  \n",
    "  if counter<50:\n",
    "    for i in range(counter,50):\n",
    "      input = torch.tensor([[0]*300])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  loss = criterion(output, torch.tensor([y_true]))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def rnn_test(review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  with torch.no_grad():\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        input = torch.tensor([current_model[each_word]])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter==50:\n",
    "        break\n",
    "    \n",
    "    if counter<50:\n",
    "      for i in range(counter,50):\n",
    "        input = torch.tensor([[0]*300])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOJe4NdOFwM-"
   },
   "outputs": [],
   "source": [
    "num_train = ternary_train.iloc[:,0].size\n",
    "current_loss = 0\n",
    "for i in range(0,num_train):\n",
    "  sentence = ternary_train.iloc[i].loc['review_body']\n",
    "  label = ternary_train.iloc[i].loc['label']\n",
    "  output, loss = train(label, sentence, mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbDGVUdEF8yh"
   },
   "outputs": [],
   "source": [
    "num_test = ternary_test.iloc[:,0].size\n",
    "output_list = []\n",
    "for i in range(0,num_test):\n",
    "  sentence = ternary_test.iloc[i].loc['review_body']\n",
    "  output = rnn_test(sentence, mymodel)\n",
    "  output = torch.argmax(output)\n",
    "  output = IntTensor.item(output)\n",
    "  #output = round(output.item())\n",
    "  output_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DcEDo8hHkok"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "from torch import IntTensor\n",
    "def predict3class(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        output = torch.argmax(model(X_batch))\n",
    "        prediction_list.append(IntTensor.item(output))\n",
    "    \n",
    "    return prediction_list#[round(num) for num in prediction_list]\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "  counter = 0\n",
    "  for i in range(0,len(y_true)):\n",
    "    if y_true[i]==y_pred[i]:\n",
    "      counter = counter+1\n",
    "  return counter/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-4sGiF1Hkx7",
    "outputId": "b4f8ed66-aa43-4beb-86a2-08309aa08dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ternary data + My model, the accuracy for recurrent neural network is 0.4054\n"
     ]
    }
   ],
   "source": [
    "y_true = ternary_test['label'].tolist()\n",
    "accuracy = get_accuracy(y_true, output_list)\n",
    "print(\"For Ternary data + My model, the accuracy for recurrent neural network is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfh8SpSTKdwu"
   },
   "source": [
    "#### Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QequP9v6Khdw"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        #output = torch.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84PTouKhKhdx"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "rnn = RNN(300, n_hidden, 3)\n",
    "LEARNING_RATE = 0.002\n",
    "criterion = nn.CrossEntropyLoss() #nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwNklDDwKhdy"
   },
   "outputs": [],
   "source": [
    "def train(y_true, review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  optimizer.zero_grad()\n",
    "  words = review.split()\n",
    "  counter = 0\n",
    "  for each_word in words:\n",
    "    try:\n",
    "      input = torch.tensor([current_model[each_word]])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "      counter = counter+1\n",
    "    except:\n",
    "      pass\n",
    "    if counter==50:\n",
    "      break\n",
    "  \n",
    "  if counter<50:\n",
    "    for i in range(counter,50):\n",
    "      input = torch.tensor([[0]*300])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  loss = criterion(output, torch.tensor([y_true]))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def rnn_test(review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  with torch.no_grad():\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        input = torch.tensor([current_model[each_word]])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter==50:\n",
    "        break\n",
    "    \n",
    "    if counter<50:\n",
    "      for i in range(counter,50):\n",
    "        input = torch.tensor([[0]*300])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qayrTDWKhdy"
   },
   "outputs": [],
   "source": [
    "num_train = ternary_train.iloc[:,0].size\n",
    "current_loss = 0\n",
    "for i in range(0,num_train):\n",
    "  sentence = ternary_train.iloc[i].loc['review_body']\n",
    "  label = ternary_train.iloc[i].loc['label']\n",
    "  output, loss = train(label, sentence, wv_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjmappdUKhdy"
   },
   "outputs": [],
   "source": [
    "num_test = ternary_test.iloc[:,0].size\n",
    "output_list = []\n",
    "for i in range(0,num_test):\n",
    "  sentence = ternary_test.iloc[i].loc['review_body']\n",
    "  output = rnn_test(sentence, wv_g)\n",
    "  output = torch.argmax(output)\n",
    "  output = IntTensor.item(output)\n",
    "  #output = round(output.item())\n",
    "  output_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzkgLONhKhdy"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "from torch import IntTensor\n",
    "def predict3class(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        output = torch.argmax(model(X_batch))\n",
    "        prediction_list.append(IntTensor.item(output))\n",
    "    \n",
    "    return prediction_list#[round(num) for num in prediction_list]\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "  counter = 0\n",
    "  for i in range(0,len(y_true)):\n",
    "    if y_true[i]==y_pred[i]:\n",
    "      counter = counter+1\n",
    "  return counter/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SCIFJrWKhdz",
    "outputId": "b23aca82-4d52-4e71-8b78-9206067d66a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ternary data + Pretrained model, the accuracy for recurrent neural network is 0.41\n"
     ]
    }
   ],
   "source": [
    "y_true = ternary_test['label'].tolist()\n",
    "accuracy = get_accuracy(y_true, output_list)\n",
    "print(\"For Ternary data + Pretrained model, the accuracy for recurrent neural network is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_soUp9PQy9p"
   },
   "source": [
    "#### part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykNoWHNnOjaz"
   },
   "source": [
    "#### Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c6OyXsjR6bE"
   },
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DknGyaIINacR"
   },
   "outputs": [],
   "source": [
    "class RNN_gate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_gate, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, 1)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        #output = self.softmax(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7dlgOzGOrge"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "rnn_gate = RNN_gate(300, n_hidden, 1)\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.BCELoss()#CrossEntropyLoss() #nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn_gate.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ4Crp2VOrge"
   },
   "outputs": [],
   "source": [
    "def train(y_true, review, current_model):\n",
    "  hidden = rnn_gate.initHidden()\n",
    "\n",
    "  #rnn.zero_grad()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  for each_word in words:\n",
    "    try:\n",
    "      input = torch.tensor([current_model[each_word]])\n",
    "      output, next_hidden = rnn_gate(input, hidden)\n",
    "      counter = counter+1\n",
    "    except:\n",
    "      pass\n",
    "    if counter==50:\n",
    "      break\n",
    "  \n",
    "  if counter<50:\n",
    "    for i in range(counter,50):\n",
    "      input = torch.tensor([[0]*300])\n",
    "      output, next_hidden = rnn_gate(input, hidden)\n",
    "    \n",
    "  loss = criterion(output.flatten(), torch.tensor([y_true]).float())\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def rnn_test(review, current_model):\n",
    "  hidden = rnn_gate.initHidden()\n",
    "  words = review.split()\n",
    "  #rnn.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "  counter = 0\n",
    "  with torch.no_grad():\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        input = torch.tensor([current_model[each_word]])\n",
    "        output, next_hidden = rnn_gate(input, hidden)\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter==50:\n",
    "        break\n",
    "    \n",
    "    if counter<50:\n",
    "      for i in range(counter,50):\n",
    "        input = torch.tensor([[0]*300])\n",
    "        output, next_hidden = rnn_gate(input, hidden)\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vK6QqcbsOrgf"
   },
   "outputs": [],
   "source": [
    "num_train = binary_train.iloc[:,0].size\n",
    "current_loss = 0\n",
    "for i in range(0,num_train):\n",
    "  sentence = binary_train.iloc[i].loc['review_body']\n",
    "  label = binary_train.iloc[i].loc['label']\n",
    "  output, loss = train(label, sentence, mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJW3xRMSOrgf",
    "outputId": "78b81ee7-ec07-48f5-dca5-324ef814c5c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Binary data + My model, the accuracy for recurrent neural network is 0.52725\n",
      "Other useful values are shown below:\n",
      "The precision, recall and f1-score of testing dataset are 0.5192894809880655, 0.9216748768472907, 0.664299662701935.\n"
     ]
    }
   ],
   "source": [
    "num_test = binary_test.iloc[:,0].size\n",
    "output_list = []\n",
    "for i in range(0,num_test):\n",
    "  sentence = binary_test.iloc[i].loc['review_body']\n",
    "  label = binary_test.iloc[i].loc['label']\n",
    "  output = rnn_test(sentence, mymodel)\n",
    "  output = round(output.item())\n",
    "  output_list.append(output)\n",
    "\n",
    "  \n",
    "tn, fp, fn, tp = confusion_matrix(binary_test['label'], output_list).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "print(\"For Binary data + My model, the accuracy for recurrent neural network is \"+str(accuracy))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Other useful values are shown below:\")\n",
    "answer_str = 'The precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kitj9Gg9Rpie"
   },
   "source": [
    "#### Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pxEXBVQR-Qa"
   },
   "outputs": [],
   "source": [
    "class RNN_gate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_gate, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, 1)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        #output = self.softmax(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6hyucfpR-Qb"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "rnn_gate = RNN_gate(300, n_hidden, 1)\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.BCELoss()#CrossEntropyLoss() #nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn_gate.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HgKFNnrR-Qb"
   },
   "outputs": [],
   "source": [
    "def train(y_true, review, current_model):\n",
    "  hidden = rnn_gate.initHidden()\n",
    "\n",
    "  #rnn.zero_grad()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  for each_word in words:\n",
    "    try:\n",
    "      input = torch.tensor([current_model[each_word]])\n",
    "      output, next_hidden = rnn_gate(input, hidden)\n",
    "      counter = counter+1\n",
    "    except:\n",
    "      pass\n",
    "    if counter==50:\n",
    "      break\n",
    "  \n",
    "  if counter<50:\n",
    "    for i in range(counter,50):\n",
    "      input = torch.tensor([[0]*300])\n",
    "      output, next_hidden = rnn_gate(input, hidden)\n",
    "    \n",
    "  loss = criterion(output.flatten(), torch.tensor([y_true]).float())\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def rnn_test(review, current_model):\n",
    "  hidden = rnn_gate.initHidden()\n",
    "  words = review.split()\n",
    "  #rnn.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "  counter = 0\n",
    "  with torch.no_grad():\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        input = torch.tensor([current_model[each_word]])\n",
    "        output, next_hidden = rnn_gate(input, hidden)\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter==50:\n",
    "        break\n",
    "    \n",
    "    if counter<50:\n",
    "      for i in range(counter,50):\n",
    "        input = torch.tensor([[0]*300])\n",
    "        output, next_hidden = rnn_gate(input, hidden)\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szIOZDWqR-Qc"
   },
   "outputs": [],
   "source": [
    "num_train = binary_train.iloc[:,0].size\n",
    "current_loss = 0\n",
    "for i in range(0,num_train):\n",
    "  sentence = binary_train.iloc[i].loc['review_body']\n",
    "  label = binary_train.iloc[i].loc['label']\n",
    "  output, loss = train(label, sentence, wv_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GDDiGdCR-Qc",
    "outputId": "eca550c6-b301-44a9-a5cb-da187c526728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Binary data + Pretrained model, the accuracy for recurrent neural network is 0.53\n",
      "Other useful values are shown below:\n",
      "The precision, recall and f1-score of testing dataset are 0.5217896571760604, 0.8847290640394089, 0.6564327485380117.\n"
     ]
    }
   ],
   "source": [
    "num_test = binary_test.iloc[:,0].size\n",
    "output_list = []\n",
    "for i in range(0,num_test):\n",
    "  sentence = binary_test.iloc[i].loc['review_body']\n",
    "  label = binary_test.iloc[i].loc['label']\n",
    "  output = rnn_test(sentence, wv_g)\n",
    "  output = round(output.item())\n",
    "  output_list.append(output)\n",
    "\n",
    "  \n",
    "tn, fp, fn, tp = confusion_matrix(binary_test['label'], output_list).ravel()\n",
    "accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "print(\"For Binary data + Pretrained model, the accuracy for recurrent neural network is \"+str(accuracy))\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(\"Other useful values are shown below:\")\n",
    "answer_str = 'The precision, recall and f1-score of testing dataset are '\n",
    "answer_str = answer_str+str(precision)+\", \"+str(recall)+\", \"+str(f1_score)+\".\"\n",
    "print(answer_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1eTtLA6SSn0"
   },
   "source": [
    "#### Ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkMICsEQSWNU"
   },
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TqahJfmTatO"
   },
   "outputs": [],
   "source": [
    "class RNN_gate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_gate, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, 1)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        #output = torch.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usVSqUn-TSdJ"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "rnn = RNN(300, n_hidden, 3)\n",
    "LEARNING_RATE = 0.002\n",
    "criterion = nn.CrossEntropyLoss() #nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-xNTDVDTSdJ"
   },
   "outputs": [],
   "source": [
    "def train(y_true, review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "\n",
    "  #rnn.zero_grad()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  for each_word in words:\n",
    "    try:\n",
    "      input = torch.tensor([current_model[each_word]])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "      counter = counter+1\n",
    "    except:\n",
    "      pass\n",
    "    if counter==50:\n",
    "      break\n",
    "  \n",
    "  if counter<50:\n",
    "    for i in range(counter,50):\n",
    "      input = torch.tensor([[0]*300])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  loss = criterion(output, torch.tensor([y_true]))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def rnn_test(review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  words = review.split()\n",
    "  #rnn.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "  counter = 0\n",
    "  with torch.no_grad():\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        input = torch.tensor([current_model[each_word]])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter==50:\n",
    "        break\n",
    "    \n",
    "    if counter<50:\n",
    "      for i in range(counter,50):\n",
    "        input = torch.tensor([[0]*300])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElM7-gTVTSdJ"
   },
   "outputs": [],
   "source": [
    "num_train = ternary_train.iloc[:,0].size\n",
    "current_loss = 0\n",
    "for i in range(0,num_train):\n",
    "  sentence = ternary_train.iloc[i].loc['review_body']\n",
    "  label = ternary_train.iloc[i].loc['label']\n",
    "  output, loss = train(label, sentence, mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26glK5ueTSdJ"
   },
   "outputs": [],
   "source": [
    "num_test = ternary_test.iloc[:,0].size\n",
    "output_list = []\n",
    "for i in range(0,num_test):\n",
    "  sentence = ternary_test.iloc[i].loc['review_body']\n",
    "  output = rnn_test(sentence, mymodel)\n",
    "  output = torch.argmax(output)\n",
    "  output = IntTensor.item(output)\n",
    "  #output = round(output.item())\n",
    "  output_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MRUs21pTSdJ"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "from torch import IntTensor\n",
    "def predict3class(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        output = torch.argmax(model(X_batch))\n",
    "        prediction_list.append(IntTensor.item(output))\n",
    "    \n",
    "    return prediction_list#[round(num) for num in prediction_list]\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "  counter = 0\n",
    "  for i in range(0,len(y_true)):\n",
    "    if y_true[i]==y_pred[i]:\n",
    "      counter = counter+1\n",
    "  return counter/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAywnPnnTSdK",
    "outputId": "6c99807e-7a17-43bb-f594-3bb81f7fb80f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ternary data + My model, the accuracy for recurrent neural network is 0.4054\n"
     ]
    }
   ],
   "source": [
    "y_true = ternary_test['label'].tolist()\n",
    "accuracy = get_accuracy(y_true, output_list)\n",
    "print(\"For Ternary data + My model, the accuracy for recurrent neural network is \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fajMpNc_SWWt"
   },
   "source": [
    "#### Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtNkfShrT3OE"
   },
   "outputs": [],
   "source": [
    "class RNN_gate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_gate, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, 1)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        #output = torch.sigmoid(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_Y9zbzQT3OE"
   },
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "rnn = RNN(300, n_hidden, 3)\n",
    "LEARNING_RATE = 0.002\n",
    "criterion = nn.CrossEntropyLoss() #nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPN1LDORT3OE"
   },
   "outputs": [],
   "source": [
    "def train(y_true, review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "\n",
    "  #rnn.zero_grad()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  words = review.split()\n",
    "\n",
    "  counter = 0\n",
    "  for each_word in words:\n",
    "    try:\n",
    "      input = torch.tensor([current_model[each_word]])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "      counter = counter+1\n",
    "    except:\n",
    "      pass\n",
    "    if counter==50:\n",
    "      break\n",
    "  \n",
    "  if counter<50:\n",
    "    for i in range(counter,50):\n",
    "      input = torch.tensor([[0]*300])\n",
    "      output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  loss = criterion(output, torch.tensor([y_true]))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return output, loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def rnn_test(review, current_model):\n",
    "  hidden = rnn.initHidden()\n",
    "  words = review.split()\n",
    "  #rnn.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "  counter = 0\n",
    "  with torch.no_grad():\n",
    "    for each_word in words:\n",
    "      try:\n",
    "        input = torch.tensor([current_model[each_word]])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "        counter = counter+1\n",
    "      except:\n",
    "        pass\n",
    "      if counter==50:\n",
    "        break\n",
    "    \n",
    "    if counter<50:\n",
    "      for i in range(counter,50):\n",
    "        input = torch.tensor([[0]*300])\n",
    "        output, next_hidden = rnn(input, hidden)\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHsMg3tuT3OE"
   },
   "outputs": [],
   "source": [
    "num_train = ternary_train.iloc[:,0].size\n",
    "current_loss = 0\n",
    "for i in range(0,num_train):\n",
    "  sentence = ternary_train.iloc[i].loc['review_body']\n",
    "  label = ternary_train.iloc[i].loc['label']\n",
    "  output, loss = train(label, sentence, wv_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbFyyzvXT3OF"
   },
   "outputs": [],
   "source": [
    "num_test = ternary_test.iloc[:,0].size\n",
    "output_list = []\n",
    "for i in range(0,num_test):\n",
    "  sentence = ternary_test.iloc[i].loc['review_body']\n",
    "  output = rnn_test(sentence, wv_g)\n",
    "  output = torch.argmax(output)\n",
    "  output = IntTensor.item(output)\n",
    "  #output = round(output.item())\n",
    "  output_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_RwNowNT3OF"
   },
   "outputs": [],
   "source": [
    "# predict test data\n",
    "from torch import IntTensor\n",
    "def predict3class(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        output = torch.argmax(model(X_batch))\n",
    "        prediction_list.append(IntTensor.item(output))\n",
    "    \n",
    "    return prediction_list#[round(num) for num in prediction_list]\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "  counter = 0\n",
    "  for i in range(0,len(y_true)):\n",
    "    if y_true[i]==y_pred[i]:\n",
    "      counter = counter+1\n",
    "  return counter/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urWSg963T3OF",
    "outputId": "709f21ca-30ee-4872-9413-3df9a1585b02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ternary data + Pretrained model, the accuracy for recurrent neural network is 0.409\n"
     ]
    }
   ],
   "source": [
    "y_true = ternary_test['label'].tolist()\n",
    "accuracy = get_accuracy(y_true, output_list)\n",
    "print(\"For Ternary data + Pretrained model, the accuracy for recurrent neural network is \"+str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "i0sIzSYDQy0w",
    "SpX34gJ7Fpq6",
    "WSI9X1H3DVdP",
    "3_soUp9PQy9p",
    "ykNoWHNnOjaz",
    "l1eTtLA6SSn0"
   ],
   "machine_shape": "hm",
   "name": "homework2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
