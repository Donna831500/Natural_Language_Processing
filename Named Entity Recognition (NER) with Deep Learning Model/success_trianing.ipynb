{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2naOOAHwjXNY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElXIVB71jhpd",
    "outputId": "d425c057-230c-4117-d5ad-c1c1fdfde8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = '/content/drive/My Drive/Colab Notebooks/HWK4_testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRZf_EQOjk2O",
    "outputId": "d694033b-c33e-4cff-a3cd-e58bf23880e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_table(path+'train',header=None,sep=' |\\n', names=['index','word','tag'], doublequote = False, keep_default_na=False) \n",
    "dev = pd.read_table(path+'dev',header=None,sep=' |\\n', names=['index','word','tag'], doublequote = False, keep_default_na=False) \n",
    "train['word'] = train['word'].astype(str)\n",
    "dev['word'] = dev['word'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G8ZzDpKiNjHl"
   },
   "outputs": [],
   "source": [
    "# ------------------------------Load Data------------------------------\n",
    "# X\n",
    "unique_words_list = sorted(list(train['word'].unique()))\n",
    "num_unique_words = len(unique_words_list)\n",
    "values = range(0,num_unique_words)\n",
    "word_index_dict = dict(zip(unique_words_list, values))  \n",
    "\n",
    "# lower_word_index_dict\n",
    "lower_unique_words_list = (map(lambda x: x.lower(), word_index_dict))\n",
    "lower_word_index_dict = dict(zip(lower_unique_words_list, values))  \n",
    "\n",
    "# add unknown words\n",
    "word_index_dict['<unk>'] = num_unique_words\n",
    "word_index_dict['<unk_digit>'] = num_unique_words+1\n",
    "word_index_dict['<unk_alnum>'] = num_unique_words+2\n",
    "\n",
    "#word_index_dict\n",
    "\n",
    "\n",
    "# embedding\n",
    "embedding = nn.Embedding(num_unique_words+3,100)  #+2 for <unk>, <unk_digit>\n",
    "torch.save(embedding.weight.data,path+'embedding_weight.pt')  #embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "ZgXa9blcXm1n",
    "outputId": "23a45013-9b44-4ce0-84c4-595986d8926f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>all_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EU</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>[-1.6467961072921753, -0.2072284072637558, 1.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rejects</td>\n",
       "      <td>O</td>\n",
       "      <td>[0.6387030482292175, 1.2546366453170776, 0.266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>German</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>[0.5691825151443481, 1.7315154075622559, 0.789...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     word     tag                                       all_features\n",
       "0      1       EU   B-ORG  [-1.6467961072921753, -0.2072284072637558, 1.9...\n",
       "1      2  rejects       O  [0.6387030482292175, 1.2546366453170776, 0.266...\n",
       "2      3   German  B-MISC  [0.5691825151443481, 1.7315154075622559, 0.789..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_features(s):\n",
    "  try:\n",
    "    lookup_tensor = torch.tensor([word_index_dict[s]], dtype=torch.long)\n",
    "    word_embed = embedding(lookup_tensor)\n",
    "    temp = word_embed.tolist()\n",
    "    feature_list = temp[0]\n",
    "  except:\n",
    "    #try:\n",
    "    #  lookup_tensor = torch.tensor([lower_word_index_dict[s]], dtype=torch.long)\n",
    "    #except:\n",
    "    if s[0].isdigit():\n",
    "      lookup_tensor = torch.tensor([word_index_dict['<unk_digit>']], dtype=torch.long)\n",
    "    elif s.isalnum():\n",
    "      lookup_tensor = torch.tensor([word_index_dict['<unk_alnum>']], dtype=torch.long)\n",
    "    else:\n",
    "      lookup_tensor = torch.tensor([word_index_dict['<unk>']], dtype=torch.long)\n",
    "    word_embed = embedding(lookup_tensor)\n",
    "    temp = word_embed.tolist()\n",
    "    feature_list = temp[0]\n",
    "    \n",
    "    #feature_list = [1]*100\n",
    "  return feature_list\n",
    "\n",
    "train['all_features'] = train['word'].apply(add_features) \n",
    "dev['all_features'] = dev['word'].apply(add_features) \n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D-AUbbntqWyU"
   },
   "outputs": [],
   "source": [
    "def getXY(index_l,feature_l):\n",
    "  all_sentences = []\n",
    "  current_sentence = []\n",
    "  for i in range(0,len(index_l)):\n",
    "    current_index = index_l[i]\n",
    "    current_feature = feature_l[i]\n",
    "    if current_index==1:\n",
    "      all_sentences.append(torch.tensor(current_sentence)) # torch.tensor(  np.array(\n",
    "      current_sentence = [current_feature]\n",
    "    else:\n",
    "      current_sentence.append(current_feature)\n",
    "  all_sentences.append(torch.tensor(current_sentence)) # torch.tensor(  np.array(\n",
    "  all_sentences = all_sentences[1:]\n",
    "  return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "54j6yQEOsub3"
   },
   "outputs": [],
   "source": [
    "# X\n",
    "train_index_list = train[\"index\"].tolist()\n",
    "train_feature_list = train[\"all_features\"].tolist()\n",
    "dev_index_list = dev[\"index\"].tolist()\n",
    "dev_feature_list = dev[\"all_features\"].tolist()\n",
    "X_train = getXY(train_index_list,train_feature_list)\n",
    "X_dev = getXY(dev_index_list,dev_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jWsJS0HNIOeD"
   },
   "outputs": [],
   "source": [
    "# Y\n",
    "Y_train_df = pd.get_dummies(train.tag, prefix='y')\n",
    "Y_dev_df = pd.get_dummies(dev.tag, prefix='y')\n",
    "#Y_train_df['y_paddding'] = 0\n",
    "#Y_dev_df['y_paddding'] = 0\n",
    "Y_train_df['target']= Y_train_df.values.tolist()\n",
    "Y_dev_df['target']= Y_dev_df.values.tolist()\n",
    "train_target_list = Y_train_df[\"target\"].tolist()\n",
    "dev_target_list = Y_dev_df[\"target\"].tolist()\n",
    "#train_target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GOnfc1DHI6oK"
   },
   "outputs": [],
   "source": [
    "Y_train = getXY(train_index_list,train_target_list)\n",
    "Y_dev = getXY(dev_index_list,dev_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AjD3ce52-l8l"
   },
   "outputs": [],
   "source": [
    "train_dataset = list(zip(X_train,Y_train))\n",
    "dev_dataset = list(zip(X_dev,Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vOTsdsT-HOfz"
   },
   "outputs": [],
   "source": [
    "\n",
    "def pad_collate(batch):\n",
    "  result = []\n",
    "  all_x_batch = []\n",
    "  all_y_batch = []\n",
    "  sentence_len_list = []\n",
    "  for each_tuple in batch:\n",
    "    (x,y) = each_tuple\n",
    "    all_x_batch.append(x)\n",
    "    all_y_batch.append(y)\n",
    "    sentence_len_list.append(len(x))\n",
    "  xx_pad = pad_sequence(all_x_batch, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(all_y_batch, batch_first=True, padding_value=0)\n",
    "  return (xx_pad,yy_pad,sentence_len_list)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,collate_fn=pad_collate, shuffle=False) \n",
    "dev_loader = DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, collate_fn=pad_collate, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hmbFNc2QQowt"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "num_layers = 1\n",
    "hidden_size = 256\n",
    "num_classes = len(train['tag'].unique()) # =9\n",
    "LEARNING_RATE = 0.003\n",
    "linear_output_size = 128\n",
    "\n",
    "# Create a bidirectional LSTM\n",
    "class BRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, num_classes):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.33)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(p=0.33)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, linear_output_size)\n",
    "        self.ELU = nn.ELU() #alpha=1.0, inplace=False\n",
    "        self.fc2 = nn.Linear(linear_output_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        #out = self.fc(out[:, -1, :])\n",
    "\n",
    "        x = self.dropout2(out[:, -1, :])\n",
    "        x = self.fc1(out) #x\n",
    "        x = self.ELU(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l08D_GTPhKk6"
   },
   "outputs": [],
   "source": [
    "b_model = BRNN(embedding_dim, hidden_size, num_layers, num_classes).to(device)\n",
    "optimizer = torch.optim.SGD(b_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DE6fQhGYdhkS"
   },
   "outputs": [],
   "source": [
    "def my_loss(output, target,sentence_l):\n",
    "  (batch_N,max_l,num_labels) = output.shape\n",
    "\n",
    "  m = nn.Softmax(dim=2)\n",
    "  temp = m(output)\n",
    "  temp = torch.log(temp)\n",
    "  #print(temp.shape)\n",
    "  #print(target.shape)\n",
    "  temp = temp*target\n",
    "  loss = torch.sum(temp)\n",
    "  return loss*(-40)/sum(sentence_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_f3Y1pH6QozC",
    "outputId": "1b4af272-88c2-4ff7-9256-dcc8891d3af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.793117).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.83420 | Validation Loss: 0.79312\n",
      "Validation loss decreased (0.793117 --> 0.720971).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.73533 | Validation Loss: 0.72097\n",
      "Validation loss decreased (0.720971 --> 0.636146).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.66153 | Validation Loss: 0.63615\n",
      "Validation loss decreased (0.636146 --> 0.579246).  Saving model ...\n",
      "Epoch 003: | Train Loss: 0.59914 | Validation Loss: 0.57925\n",
      "Validation loss decreased (0.579246 --> 0.528334).  Saving model ...\n",
      "Epoch 004: | Train Loss: 0.55464 | Validation Loss: 0.52833\n",
      "Validation loss decreased (0.528334 --> 0.495243).  Saving model ...\n",
      "Epoch 005: | Train Loss: 0.51981 | Validation Loss: 0.49524\n",
      "Validation loss decreased (0.495243 --> 0.462558).  Saving model ...\n",
      "Epoch 006: | Train Loss: 0.48971 | Validation Loss: 0.46256\n",
      "Validation loss decreased (0.462558 --> 0.419540).  Saving model ...\n",
      "Epoch 007: | Train Loss: 0.46357 | Validation Loss: 0.41954\n",
      "Epoch 008: | Train Loss: 0.43810 | Validation Loss: 0.42214\n",
      "Validation loss decreased (0.419540 --> 0.388356).  Saving model ...\n",
      "Epoch 009: | Train Loss: 0.41667 | Validation Loss: 0.38836\n",
      "Validation loss decreased (0.388356 --> 0.372609).  Saving model ...\n",
      "Epoch 010: | Train Loss: 0.39549 | Validation Loss: 0.37261\n",
      "Validation loss decreased (0.372609 --> 0.359777).  Saving model ...\n",
      "Epoch 011: | Train Loss: 0.37482 | Validation Loss: 0.35978\n",
      "Validation loss decreased (0.359777 --> 0.344564).  Saving model ...\n",
      "Epoch 012: | Train Loss: 0.35357 | Validation Loss: 0.34456\n",
      "Validation loss decreased (0.344564 --> 0.329464).  Saving model ...\n",
      "Epoch 013: | Train Loss: 0.33781 | Validation Loss: 0.32946\n",
      "Validation loss decreased (0.329464 --> 0.310461).  Saving model ...\n",
      "Epoch 014: | Train Loss: 0.31989 | Validation Loss: 0.31046\n",
      "Epoch 015: | Train Loss: 0.30437 | Validation Loss: 0.31221\n",
      "Validation loss decreased (0.310461 --> 0.301082).  Saving model ...\n",
      "Epoch 016: | Train Loss: 0.28883 | Validation Loss: 0.30108\n",
      "Epoch 017: | Train Loss: 0.27572 | Validation Loss: 0.30187\n",
      "Validation loss decreased (0.301082 --> 0.264118).  Saving model ...\n",
      "Epoch 018: | Train Loss: 0.26353 | Validation Loss: 0.26412\n",
      "Epoch 019: | Train Loss: 0.25244 | Validation Loss: 0.26712\n",
      "Validation loss decreased (0.264118 --> 0.255503).  Saving model ...\n",
      "Epoch 020: | Train Loss: 0.24248 | Validation Loss: 0.25550\n",
      "Validation loss decreased (0.255503 --> 0.237281).  Saving model ...\n",
      "Epoch 021: | Train Loss: 0.23525 | Validation Loss: 0.23728\n",
      "Epoch 022: | Train Loss: 0.22404 | Validation Loss: 0.24748\n",
      "Epoch 023: | Train Loss: 0.21817 | Validation Loss: 0.25199\n",
      "Validation loss decreased (0.237281 --> 0.225191).  Saving model ...\n",
      "Epoch 024: | Train Loss: 0.20958 | Validation Loss: 0.22519\n",
      "Epoch 025: | Train Loss: 0.20373 | Validation Loss: 0.23061\n",
      "Epoch 026: | Train Loss: 0.19651 | Validation Loss: 0.24777\n",
      "Epoch 027: | Train Loss: 0.19090 | Validation Loss: 0.23331\n",
      "Epoch 028: | Train Loss: 0.18541 | Validation Loss: 0.23091\n",
      "Epoch 029: | Train Loss: 0.17959 | Validation Loss: 0.22775\n",
      "Epoch 030: | Train Loss: 0.17394 | Validation Loss: 0.23567\n",
      "Validation loss decreased (0.225191 --> 0.204170).  Saving model ...\n",
      "Epoch 031: | Train Loss: 0.17088 | Validation Loss: 0.20417\n",
      "Validation loss decreased (0.204170 --> 0.204163).  Saving model ...\n",
      "Epoch 032: | Train Loss: 0.16454 | Validation Loss: 0.20416\n",
      "Epoch 033: | Train Loss: 0.16174 | Validation Loss: 0.20484\n",
      "Epoch 034: | Train Loss: 0.15780 | Validation Loss: 0.21274\n",
      "Epoch 035: | Train Loss: 0.15332 | Validation Loss: 0.21855\n",
      "Validation loss decreased (0.204163 --> 0.189532).  Saving model ...\n",
      "Epoch 036: | Train Loss: 0.15021 | Validation Loss: 0.18953\n",
      "Epoch 037: | Train Loss: 0.14725 | Validation Loss: 0.19116\n",
      "Epoch 038: | Train Loss: 0.14451 | Validation Loss: 0.20518\n",
      "Validation loss decreased (0.189532 --> 0.188520).  Saving model ...\n",
      "Epoch 039: | Train Loss: 0.14070 | Validation Loss: 0.18852\n",
      "Epoch 040: | Train Loss: 0.13949 | Validation Loss: 0.19769\n",
      "Epoch 041: | Train Loss: 0.13632 | Validation Loss: 0.19114\n",
      "Epoch 042: | Train Loss: 0.13481 | Validation Loss: 0.19851\n",
      "Epoch 043: | Train Loss: 0.12980 | Validation Loss: 0.19564\n",
      "Epoch 044: | Train Loss: 0.12907 | Validation Loss: 0.21272\n",
      "Epoch 045: | Train Loss: 0.12617 | Validation Loss: 0.19681\n",
      "Epoch 046: | Train Loss: 0.12456 | Validation Loss: 0.19885\n",
      "Epoch 047: | Train Loss: 0.12021 | Validation Loss: 0.20904\n",
      "Epoch 048: | Train Loss: 0.12121 | Validation Loss: 0.19594\n",
      "Epoch 049: | Train Loss: 0.11841 | Validation Loss: 0.19867\n",
      "Epoch 050: | Train Loss: 0.11730 | Validation Loss: 0.19631\n",
      "Epoch 051: | Train Loss: 0.11500 | Validation Loss: 0.22120\n",
      "Epoch 052: | Train Loss: 0.11449 | Validation Loss: 0.20327\n",
      "Epoch 053: | Train Loss: 0.11354 | Validation Loss: 0.19329\n",
      "Epoch 054: | Train Loss: 0.10880 | Validation Loss: 0.20674\n",
      "Epoch 055: | Train Loss: 0.10637 | Validation Loss: 0.20624\n",
      "Epoch 056: | Train Loss: 0.10721 | Validation Loss: 0.19786\n",
      "Epoch 057: | Train Loss: 0.10435 | Validation Loss: 0.21865\n",
      "Epoch 058: | Train Loss: 0.10364 | Validation Loss: 0.20141\n",
      "Epoch 059: | Train Loss: 0.10248 | Validation Loss: 0.20478\n",
      "Epoch 060: | Train Loss: 0.10248 | Validation Loss: 0.21488\n",
      "Epoch 061: | Train Loss: 0.09991 | Validation Loss: 0.21924\n",
      "Epoch 062: | Train Loss: 0.09788 | Validation Loss: 0.20402\n",
      "Epoch 063: | Train Loss: 0.09691 | Validation Loss: 0.20805\n",
      "Epoch 064: | Train Loss: 0.09579 | Validation Loss: 0.22191\n",
      "Epoch 065: | Train Loss: 0.09558 | Validation Loss: 0.21665\n",
      "Epoch 066: | Train Loss: 0.09362 | Validation Loss: 0.20957\n",
      "Epoch 067: | Train Loss: 0.09196 | Validation Loss: 0.20939\n",
      "Epoch 068: | Train Loss: 0.09067 | Validation Loss: 0.19318\n",
      "Epoch 069: | Train Loss: 0.09202 | Validation Loss: 0.19555\n",
      "Epoch 070: | Train Loss: 0.08703 | Validation Loss: 0.19202\n",
      "Epoch 071: | Train Loss: 0.08923 | Validation Loss: 0.22598\n",
      "Epoch 072: | Train Loss: 0.08645 | Validation Loss: 0.19968\n",
      "Epoch 073: | Train Loss: 0.08761 | Validation Loss: 0.21463\n",
      "Epoch 074: | Train Loss: 0.08447 | Validation Loss: 0.21020\n",
      "Epoch 075: | Train Loss: 0.08393 | Validation Loss: 0.20469\n",
      "Epoch 076: | Train Loss: 0.08241 | Validation Loss: 0.19968\n",
      "Epoch 077: | Train Loss: 0.08185 | Validation Loss: 0.19408\n",
      "Epoch 078: | Train Loss: 0.08289 | Validation Loss: 0.20209\n",
      "Epoch 079: | Train Loss: 0.08045 | Validation Loss: 0.20899\n",
      "Epoch 080: | Train Loss: 0.08051 | Validation Loss: 0.20471\n",
      "Epoch 081: | Train Loss: 0.08171 | Validation Loss: 0.20581\n",
      "Epoch 082: | Train Loss: 0.07984 | Validation Loss: 0.23378\n",
      "Epoch 083: | Train Loss: 0.07886 | Validation Loss: 0.20071\n",
      "Epoch 084: | Train Loss: 0.07701 | Validation Loss: 0.22657\n",
      "Epoch 085: | Train Loss: 0.07728 | Validation Loss: 0.21795\n",
      "Epoch 086: | Train Loss: 0.07771 | Validation Loss: 0.20437\n",
      "Epoch 087: | Train Loss: 0.07663 | Validation Loss: 0.20074\n",
      "Epoch 088: | Train Loss: 0.07431 | Validation Loss: 0.19101\n",
      "Epoch 089: | Train Loss: 0.07447 | Validation Loss: 0.21216\n",
      "Epoch 090: | Train Loss: 0.07439 | Validation Loss: 0.20797\n",
      "Epoch 091: | Train Loss: 0.07397 | Validation Loss: 0.20270\n",
      "Epoch 092: | Train Loss: 0.07302 | Validation Loss: 0.20376\n",
      "Epoch 093: | Train Loss: 0.07251 | Validation Loss: 0.20410\n",
      "Epoch 094: | Train Loss: 0.07114 | Validation Loss: 0.20274\n",
      "Epoch 095: | Train Loss: 0.06982 | Validation Loss: 0.20204\n",
      "Epoch 096: | Train Loss: 0.06885 | Validation Loss: 0.19785\n",
      "Epoch 097: | Train Loss: 0.06983 | Validation Loss: 0.20739\n",
      "Validation loss decreased (0.188520 --> 0.187487).  Saving model ...\n",
      "Epoch 098: | Train Loss: 0.06871 | Validation Loss: 0.18749\n",
      "Epoch 099: | Train Loss: 0.06887 | Validation Loss: 0.21918\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "\n",
    "ceof = 40\n",
    "N_EPOCHS = 100\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "  train_loss = 0\n",
    "  valid_loss = 0\n",
    "  b_model.train()\n",
    "  for X_batch, y_batch,sll in train_loader:\n",
    "    X_batch = X_batch.float()\n",
    "    optimizer.zero_grad()\n",
    "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "    y_pred = b_model(X_batch)\n",
    "\n",
    "    loss = my_loss(y_pred, y_batch,sll)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "\n",
    "  b_model.eval() # prep model for evaluation\n",
    "  for data, target, sll in dev_loader:\n",
    "      data = data.float()\n",
    "\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = b_model(data)\n",
    "      # calculate the loss\n",
    "      loss = my_loss(output, target,sll)\n",
    "      valid_loss += loss.item()\n",
    "\n",
    "  if (valid_loss/len(dev_loader)) <= valid_loss_min:\n",
    "      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "      valid_loss_min/ceof,\n",
    "      valid_loss/(len(dev_loader)*ceof)) )\n",
    "      torch.save(b_model.state_dict(), path+'model.pt')\n",
    "      valid_loss_min = valid_loss/len(dev_loader)\n",
    "      \n",
    "\n",
    "  print(f'Epoch {e+0:03}: | Train Loss: {train_loss/(len(train_loader)*ceof):.5f} | Validation Loss: {valid_loss/(len(dev_loader)*ceof):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_H8egqQgk1n",
    "outputId": "e2a1a50e-3849-452e-f2c6-7a8c41ad4ae2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state of model\n",
    "b_model.load_state_dict(torch.load(path+'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oN0kVR5EEOby"
   },
   "outputs": [],
   "source": [
    "# predict dev data\n",
    "num_labels = len(train['tag'].unique())\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    temp = list(Y_dev_df)\n",
    "    tag_list = []\n",
    "    for each_name in temp:\n",
    "      tag_list.append(each_name[2:])\n",
    "\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch,y,sll in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        X_batch, y = X_batch.to(device), y.to(device)\n",
    "        output = model(X_batch)\n",
    "        temp = output.tolist()\n",
    "\n",
    "        for i in range(0,len(temp)):\n",
    "          current_s_output = temp[i]\n",
    "          current_s_len = sll[i]\n",
    "          for j in range(0,current_s_len):\n",
    "            current_w_output = current_s_output[j]\n",
    "            max_index = current_w_output.index(max(current_w_output[:num_labels]))\n",
    "            prediction_list.append(tag_list[max_index])\n",
    "    \n",
    "    return prediction_list\n",
    "  \n",
    "all_prediction = predict(b_model, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0EhjiQimMmeh"
   },
   "outputs": [],
   "source": [
    "dev['pred'] = all_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EPaRxW0u5r8I"
   },
   "outputs": [],
   "source": [
    "# output to greedy.out\n",
    "\n",
    "t1_output_df = dev.drop('all_features', 1)\n",
    "t1_output_list = t1_output_df.values.tolist()\n",
    "first_flag = True\n",
    "with open(path+'dev1.out', 'w') as f:\n",
    "    for each_output in t1_output_list:\n",
    "        idx = each_output[0]\n",
    "        word = each_output[1]\n",
    "        gold = each_output[2]\n",
    "        pred = each_output[3]\n",
    "\n",
    "        if idx==1:\n",
    "            if first_flag:\n",
    "                first_flag = False\n",
    "            else:\n",
    "                f.write('\\n')\n",
    "        try:\n",
    "          f.write(str(idx)+' '+word+' '+gold+' '+pred)\n",
    "          f.write('\\n')\n",
    "        except:\n",
    "          print(counter)\n",
    "          print(idx)\n",
    "          print(word)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hkk9VeakSnX8"
   },
   "outputs": [],
   "source": [
    "torch.save(b_model, path+'blstm1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyyFU41mo3Wk"
   },
   "outputs": [],
   "source": [
    "########################################## testing task 1 ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8C8LRLe2TwOr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xEbcqP3SQE-6"
   },
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#path = '/content/drive/My Drive/Colab Notebooks/HWK4_testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hktqJ6xiQX2B",
    "outputId": "e6e3fa4e-798f-4cb6-cef8-7b8def5832cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_table(path+'train',header=None,sep=' |\\n', names=['index','word','tag'], doublequote = False, keep_default_na=False) \n",
    "dev = pd.read_table(path+'dev',header=None,sep=' |\\n', names=['index','word','tag'], doublequote = False, keep_default_na=False) \n",
    "train['word'] = train['word'].astype(str)\n",
    "dev['word'] = dev['word'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZNHDljxMQX4F"
   },
   "outputs": [],
   "source": [
    "# ------------------------------Load Data------------------------------\n",
    "# X\n",
    "unique_words_list = sorted(list(train['word'].unique()))\n",
    "num_unique_words = len(unique_words_list)\n",
    "values = range(0,num_unique_words)\n",
    "word_index_dict = dict(zip(unique_words_list, values))  \n",
    "\n",
    "# lower_word_index_dict\n",
    "lower_unique_words_list = (map(lambda x: x.lower(), word_index_dict))\n",
    "lower_word_index_dict = dict(zip(lower_unique_words_list, values))  \n",
    "\n",
    "# add unknown words\n",
    "word_index_dict['<unk>'] = num_unique_words\n",
    "word_index_dict['<unk_digit>'] = num_unique_words+1\n",
    "word_index_dict['<unk_alnum>'] = num_unique_words+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_w433D45QX67"
   },
   "outputs": [],
   "source": [
    "# load word embedding\n",
    "embedding_weight = torch.load(path+'embedding_weight.pt')\n",
    "embedding = nn.Embedding.from_pretrained(embedding_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NmvuOmQkQX8-"
   },
   "outputs": [],
   "source": [
    "def add_features(s):\n",
    "  try:\n",
    "    lookup_tensor = torch.tensor([word_index_dict[s]], dtype=torch.long)\n",
    "    word_embed = embedding(lookup_tensor)\n",
    "    temp = word_embed.tolist()\n",
    "    feature_list = temp[0]\n",
    "  except:\n",
    "    #try:\n",
    "    #  lookup_tensor = torch.tensor([lower_word_index_dict[s]], dtype=torch.long)\n",
    "    #except:\n",
    "    if s[0].isdigit():\n",
    "      lookup_tensor = torch.tensor([word_index_dict['<unk_digit>']], dtype=torch.long)\n",
    "    elif s.isalnum():\n",
    "      lookup_tensor = torch.tensor([word_index_dict['<unk_alnum>']], dtype=torch.long)\n",
    "    else:\n",
    "      lookup_tensor = torch.tensor([word_index_dict['<unk>']], dtype=torch.long)\n",
    "    word_embed = embedding(lookup_tensor)\n",
    "    temp = word_embed.tolist()\n",
    "    feature_list = temp[0]\n",
    "    \n",
    "    #feature_list = [1]*100\n",
    "  return feature_list\n",
    "\n",
    "train['all_features'] = train['word'].apply(add_features) \n",
    "dev['all_features'] = dev['word'].apply(add_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mLOrTEKPQqBA"
   },
   "outputs": [],
   "source": [
    "def getXY(index_l,feature_l):\n",
    "  all_sentences = []\n",
    "  current_sentence = []\n",
    "  for i in range(0,len(index_l)):\n",
    "    current_index = index_l[i]\n",
    "    current_feature = feature_l[i]\n",
    "    if current_index==1:\n",
    "      all_sentences.append(torch.tensor(current_sentence)) # torch.tensor(  np.array(\n",
    "      current_sentence = [current_feature]\n",
    "    else:\n",
    "      current_sentence.append(current_feature)\n",
    "  all_sentences.append(torch.tensor(current_sentence)) # torch.tensor(  np.array(\n",
    "  all_sentences = all_sentences[1:]\n",
    "  return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wFaeAL4_QqDq"
   },
   "outputs": [],
   "source": [
    "# X\n",
    "train_index_list = train[\"index\"].tolist()\n",
    "train_feature_list = train[\"all_features\"].tolist()\n",
    "dev_index_list = dev[\"index\"].tolist()\n",
    "dev_feature_list = dev[\"all_features\"].tolist()\n",
    "X_train = getXY(train_index_list,train_feature_list)\n",
    "X_dev = getXY(dev_index_list,dev_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FBYYm5fdQv-T"
   },
   "outputs": [],
   "source": [
    "# Y\n",
    "Y_train_df = pd.get_dummies(train.tag, prefix='y')\n",
    "Y_dev_df = pd.get_dummies(dev.tag, prefix='y')\n",
    "#Y_train_df['y_paddding'] = 0\n",
    "#Y_dev_df['y_paddding'] = 0\n",
    "Y_train_df['target']= Y_train_df.values.tolist()\n",
    "Y_dev_df['target']= Y_dev_df.values.tolist()\n",
    "train_target_list = Y_train_df[\"target\"].tolist()\n",
    "dev_target_list = Y_dev_df[\"target\"].tolist()\n",
    "#train_target_list\n",
    "Y_train = getXY(train_index_list,train_target_list)\n",
    "Y_dev = getXY(dev_index_list,dev_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LYuUNfk0QqFv"
   },
   "outputs": [],
   "source": [
    "train_dataset = list(zip(X_train,Y_train))\n",
    "dev_dataset = list(zip(X_dev,Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PcFC4415Q45B"
   },
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "  result = []\n",
    "  all_x_batch = []\n",
    "  all_y_batch = []\n",
    "  sentence_len_list = []\n",
    "  for each_tuple in batch:\n",
    "    (x,y) = each_tuple\n",
    "    all_x_batch.append(x)\n",
    "    all_y_batch.append(y)\n",
    "    sentence_len_list.append(len(x))\n",
    "  xx_pad = pad_sequence(all_x_batch, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(all_y_batch, batch_first=True, padding_value=0)\n",
    "  return (xx_pad,yy_pad,sentence_len_list)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,collate_fn=pad_collate, shuffle=False) \n",
    "dev_loader = DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, collate_fn=pad_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_YT6QEQtQ5q2"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "num_layers = 1\n",
    "hidden_size = 256\n",
    "num_classes = len(train['tag'].unique()) # =9\n",
    "LEARNING_RATE = 0.003\n",
    "linear_output_size = 128\n",
    "\n",
    "# Create a bidirectional LSTM\n",
    "class BRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, num_classes):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.33)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(p=0.33)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, linear_output_size)\n",
    "        self.ELU = nn.ELU() #alpha=1.0, inplace=False\n",
    "        self.fc2 = nn.Linear(linear_output_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        #out = self.fc(out[:, -1, :])\n",
    "\n",
    "        x = self.dropout2(out[:, -1, :])\n",
    "        x = self.fc1(out) #x\n",
    "        x = self.ELU(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ohoxVCHaQ493"
   },
   "outputs": [],
   "source": [
    "brnn_model = torch.load(path+'blstm1.pt')\n",
    "brnn_model = brnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jmKpaVGQQ5Ak"
   },
   "outputs": [],
   "source": [
    "# predict dev data\n",
    "num_labels = len(train['tag'].unique())\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    temp = list(Y_dev_df)\n",
    "    tag_list = []\n",
    "    for each_name in temp:\n",
    "      tag_list.append(each_name[2:])\n",
    "\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "      for X_batch,y,sll in dataloader:\n",
    "        X_batch = X_batch.float()\n",
    "        X_batch, y = X_batch.to(device), y.to(device)\n",
    "        output = model(X_batch)\n",
    "        temp = output.tolist()\n",
    "\n",
    "        for i in range(0,len(temp)):\n",
    "          current_s_output = temp[i]\n",
    "          current_s_len = sll[i]\n",
    "          for j in range(0,current_s_len):\n",
    "            current_w_output = current_s_output[j]\n",
    "            max_index = current_w_output.index(max(current_w_output[:num_labels]))\n",
    "            prediction_list.append(tag_list[max_index])\n",
    "    \n",
    "    return prediction_list\n",
    "  \n",
    "all_prediction = predict(brnn_model, dev_loader)\n",
    "dev['pred'] = all_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JXR6X_NdTLM0"
   },
   "outputs": [],
   "source": [
    "# output to task 1\n",
    "t1_output_df = dev.drop('all_features', 1)\n",
    "t1_output_list = t1_output_df.values.tolist()\n",
    "first_flag = True\n",
    "with open(path+'dev1.out', 'w') as f:\n",
    "    for each_output in t1_output_list:\n",
    "        idx = each_output[0]\n",
    "        word = each_output[1]\n",
    "        gold = each_output[2]\n",
    "        pred = each_output[3]\n",
    "\n",
    "        if idx==1:\n",
    "            if first_flag:\n",
    "                first_flag = False\n",
    "            else:\n",
    "                f.write('\\n')\n",
    "        try:\n",
    "          f.write(str(idx)+' '+word+' '+gold+' '+pred)\n",
    "          f.write('\\n')\n",
    "        except:\n",
    "          print(counter)\n",
    "          print(idx)\n",
    "          print(word)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyAd7AdITLPO"
   },
   "outputs": [],
   "source": [
    "############################################## task 2 start #######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgNkHcBAqYo0"
   },
   "outputs": [],
   "source": [
    "################################################# TASK 2 ############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6PJrdZ-qJe8",
    "outputId": "ae219301-73e0-41d6-9f99-03cb23307ed5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------Load GloVe------------------------------\n",
    "GloVe = pd.read_table(path+'glove.6B.100d.txt',header=None,sep=' |\\n', doublequote = False) \n",
    "GloVe[0] = GloVe[0].astype(str)\n",
    "GloVe_word_list= GloVe[0].tolist()\n",
    "GloVe.drop(GloVe.columns[0], axis=1, inplace=True)\n",
    "GloVe['target']= GloVe.values.tolist()\n",
    "GloVe_target_list = GloVe['target'].tolist()\n",
    "#[str(i) for i in lst]\n",
    "GloVe_word_list = [str(i) for i in GloVe_word_list]\n",
    "GloVe_word_index_dict = dict(zip(GloVe_word_list, GloVe_target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nnYmhuhYq1Io"
   },
   "outputs": [],
   "source": [
    "GloVe_lower_word_list = (map(lambda x: x.lower(), GloVe_word_list))\n",
    "GloVe_lowcase_word_index_dict = dict(zip(GloVe_lower_word_list, GloVe_target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "G-AJ2uHwAg21"
   },
   "outputs": [],
   "source": [
    "def add_features(s):\n",
    "  try:\n",
    "    feature_list = GloVe_word_index_dict[s]\n",
    "  except:\n",
    "    try:\n",
    "      feature_list = GloVe_lowcase_word_index_dict[s.lower()]\n",
    "    except:\n",
    "      if s[0].isdigit():\n",
    "        lookup_tensor = torch.tensor([word_index_dict['<unk_digit>']], dtype=torch.long)\n",
    "      elif s.isalnum():\n",
    "        lookup_tensor = torch.tensor([word_index_dict['<unk_alnum>']], dtype=torch.long)\n",
    "      else:\n",
    "        lookup_tensor = torch.tensor([word_index_dict['<unk>']], dtype=torch.long)\n",
    "      word_embed = embedding(lookup_tensor)\n",
    "      temp = word_embed.tolist()\n",
    "      feature_list = temp[0]\n",
    "    \n",
    "    #feature_list = [1]*100\n",
    "  return feature_list\n",
    "\n",
    "train['all_features'] = train['word'].apply(add_features) \n",
    "dev['all_features'] = dev['word'].apply(add_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AlcLRQCOCCtC"
   },
   "outputs": [],
   "source": [
    "# X\n",
    "train_index_list = train[\"index\"].tolist()\n",
    "train_feature_list = train[\"all_features\"].tolist()\n",
    "dev_index_list = dev[\"index\"].tolist()\n",
    "dev_feature_list = dev[\"all_features\"].tolist()\n",
    "X_train = getXY(train_index_list,train_feature_list)\n",
    "X_dev = getXY(dev_index_list,dev_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UHvJzIheCWKB"
   },
   "outputs": [],
   "source": [
    "# Y\n",
    "Y_train_df = pd.get_dummies(train.tag, prefix='y')\n",
    "Y_dev_df = pd.get_dummies(dev.tag, prefix='y')\n",
    "#Y_train_df['y_paddding'] = 0\n",
    "#Y_dev_df['y_paddding'] = 0\n",
    "Y_train_df['target']= Y_train_df.values.tolist()\n",
    "Y_dev_df['target']= Y_dev_df.values.tolist()\n",
    "train_target_list = Y_train_df[\"target\"].tolist()\n",
    "dev_target_list = Y_dev_df[\"target\"].tolist()\n",
    "#train_target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NbV73Ab6CYuL"
   },
   "outputs": [],
   "source": [
    "Y_train = getXY(train_index_list,train_target_list)\n",
    "Y_dev = getXY(dev_index_list,dev_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "M1baP-YlCdya"
   },
   "outputs": [],
   "source": [
    "train_dataset = list(zip(X_train,Y_train))\n",
    "dev_dataset = list(zip(X_dev,Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rW2rT4BZCgZg"
   },
   "outputs": [],
   "source": [
    "#################### data loader ####################\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,collate_fn=pad_collate, shuffle=False) \n",
    "dev_loader = DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, collate_fn=pad_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TNbotccyCmxR"
   },
   "outputs": [],
   "source": [
    "b_model_2 = BRNN(embedding_dim, hidden_size, num_layers, num_classes).to(device)\n",
    "optimizer_2 = torch.optim.SGD(b_model_2.parameters(), lr=LEARNING_RATE)\n",
    "LEARNING_RATE = 0.029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "5RNGrU7lIDdg"
   },
   "outputs": [],
   "source": [
    "def my_loss_2(output, target,sentence_l):\n",
    "  (batch_N,max_l,num_labels) = output.shape\n",
    "\n",
    "  m = nn.Softmax(dim=2)\n",
    "  temp = m(output)\n",
    "  temp = torch.log(temp)\n",
    "  temp = temp*target\n",
    "  loss = torch.sum(temp)\n",
    "  return loss*(-20)/sum(sentence_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2J9r_zqCu0p",
    "outputId": "f14ef7e4-352f-48b5-a912-a88ead7b878c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.307884).  Saving model ...\n",
      "Epoch 000: | Train Loss: 0.46346 | Validation Loss: 0.30788\n",
      "Validation loss decreased (0.307884 --> 0.219940).  Saving model ...\n",
      "Epoch 001: | Train Loss: 0.27547 | Validation Loss: 0.21994\n",
      "Validation loss decreased (0.219940 --> 0.181630).  Saving model ...\n",
      "Epoch 002: | Train Loss: 0.21868 | Validation Loss: 0.18163\n",
      "Validation loss decreased (0.181630 --> 0.153190).  Saving model ...\n",
      "Epoch 003: | Train Loss: 0.18602 | Validation Loss: 0.15319\n",
      "Epoch 004: | Train Loss: 0.16464 | Validation Loss: 0.15631\n",
      "Validation loss decreased (0.153190 --> 0.147467).  Saving model ...\n",
      "Epoch 005: | Train Loss: 0.15225 | Validation Loss: 0.14747\n",
      "Validation loss decreased (0.147467 --> 0.132127).  Saving model ...\n",
      "Epoch 006: | Train Loss: 0.14199 | Validation Loss: 0.13213\n",
      "Validation loss decreased (0.132127 --> 0.129463).  Saving model ...\n",
      "Epoch 007: | Train Loss: 0.13334 | Validation Loss: 0.12946\n",
      "Validation loss decreased (0.129463 --> 0.121798).  Saving model ...\n",
      "Epoch 008: | Train Loss: 0.12606 | Validation Loss: 0.12180\n",
      "Validation loss decreased (0.121798 --> 0.119903).  Saving model ...\n",
      "Epoch 009: | Train Loss: 0.12096 | Validation Loss: 0.11990\n",
      "Validation loss decreased (0.119903 --> 0.110723).  Saving model ...\n",
      "Epoch 010: | Train Loss: 0.11518 | Validation Loss: 0.11072\n",
      "Validation loss decreased (0.110723 --> 0.109574).  Saving model ...\n",
      "Epoch 011: | Train Loss: 0.11021 | Validation Loss: 0.10957\n",
      "Validation loss decreased (0.109574 --> 0.102139).  Saving model ...\n",
      "Epoch 012: | Train Loss: 0.10533 | Validation Loss: 0.10214\n",
      "Epoch 013: | Train Loss: 0.10109 | Validation Loss: 0.11990\n",
      "Epoch 014: | Train Loss: 0.09622 | Validation Loss: 0.10239\n",
      "Epoch 015: | Train Loss: 0.09398 | Validation Loss: 0.10473\n",
      "Validation loss decreased (0.102139 --> 0.090947).  Saving model ...\n",
      "Epoch 016: | Train Loss: 0.09178 | Validation Loss: 0.09095\n",
      "Validation loss decreased (0.090947 --> 0.089453).  Saving model ...\n",
      "Epoch 017: | Train Loss: 0.08725 | Validation Loss: 0.08945\n",
      "Epoch 018: | Train Loss: 0.08621 | Validation Loss: 0.09097\n",
      "Epoch 019: | Train Loss: 0.08212 | Validation Loss: 0.09324\n",
      "Validation loss decreased (0.089453 --> 0.086908).  Saving model ...\n",
      "Epoch 020: | Train Loss: 0.08037 | Validation Loss: 0.08691\n",
      "Epoch 021: | Train Loss: 0.07773 | Validation Loss: 0.09081\n",
      "Epoch 022: | Train Loss: 0.07452 | Validation Loss: 0.08819\n",
      "Epoch 023: | Train Loss: 0.07432 | Validation Loss: 0.08795\n",
      "Validation loss decreased (0.086908 --> 0.084039).  Saving model ...\n",
      "Epoch 024: | Train Loss: 0.07220 | Validation Loss: 0.08404\n",
      "Validation loss decreased (0.084039 --> 0.081400).  Saving model ...\n",
      "Epoch 025: | Train Loss: 0.06891 | Validation Loss: 0.08140\n",
      "Epoch 026: | Train Loss: 0.06840 | Validation Loss: 0.08376\n",
      "Epoch 027: | Train Loss: 0.06548 | Validation Loss: 0.08430\n",
      "Epoch 028: | Train Loss: 0.06497 | Validation Loss: 0.08433\n",
      "Epoch 029: | Train Loss: 0.06298 | Validation Loss: 0.08265\n",
      "Epoch 030: | Train Loss: 0.06141 | Validation Loss: 0.08438\n",
      "Epoch 031: | Train Loss: 0.05924 | Validation Loss: 0.08207\n",
      "Validation loss decreased (0.081400 --> 0.081024).  Saving model ...\n",
      "Epoch 032: | Train Loss: 0.05901 | Validation Loss: 0.08102\n",
      "Epoch 033: | Train Loss: 0.05713 | Validation Loss: 0.08579\n",
      "Epoch 034: | Train Loss: 0.05528 | Validation Loss: 0.08249\n",
      "Epoch 035: | Train Loss: 0.05408 | Validation Loss: 0.08345\n",
      "Epoch 036: | Train Loss: 0.05207 | Validation Loss: 0.08124\n",
      "Epoch 037: | Train Loss: 0.05204 | Validation Loss: 0.08147\n",
      "Epoch 038: | Train Loss: 0.04988 | Validation Loss: 0.08332\n",
      "Epoch 039: | Train Loss: 0.04833 | Validation Loss: 0.08483\n",
      "Epoch 040: | Train Loss: 0.04869 | Validation Loss: 0.08355\n",
      "Epoch 041: | Train Loss: 0.04631 | Validation Loss: 0.08519\n",
      "Epoch 042: | Train Loss: 0.04567 | Validation Loss: 0.08367\n",
      "Epoch 043: | Train Loss: 0.04468 | Validation Loss: 0.08333\n",
      "Epoch 044: | Train Loss: 0.04453 | Validation Loss: 0.08555\n",
      "Epoch 045: | Train Loss: 0.04299 | Validation Loss: 0.08477\n",
      "Epoch 046: | Train Loss: 0.04162 | Validation Loss: 0.08729\n",
      "Epoch 047: | Train Loss: 0.04092 | Validation Loss: 0.08815\n",
      "Epoch 048: | Train Loss: 0.03994 | Validation Loss: 0.08515\n",
      "Epoch 049: | Train Loss: 0.03957 | Validation Loss: 0.08764\n",
      "Epoch 050: | Train Loss: 0.03773 | Validation Loss: 0.08483\n",
      "Epoch 051: | Train Loss: 0.03727 | Validation Loss: 0.08462\n",
      "Epoch 052: | Train Loss: 0.03621 | Validation Loss: 0.08720\n",
      "Epoch 053: | Train Loss: 0.03561 | Validation Loss: 0.08696\n",
      "Epoch 054: | Train Loss: 0.03630 | Validation Loss: 0.08770\n",
      "Epoch 055: | Train Loss: 0.03453 | Validation Loss: 0.08850\n",
      "Epoch 056: | Train Loss: 0.03385 | Validation Loss: 0.08874\n",
      "Epoch 057: | Train Loss: 0.03254 | Validation Loss: 0.09066\n",
      "Epoch 058: | Train Loss: 0.03238 | Validation Loss: 0.09376\n",
      "Epoch 059: | Train Loss: 0.03152 | Validation Loss: 0.09191\n",
      "Epoch 060: | Train Loss: 0.03117 | Validation Loss: 0.09467\n",
      "Epoch 061: | Train Loss: 0.02904 | Validation Loss: 0.09386\n",
      "Epoch 062: | Train Loss: 0.03050 | Validation Loss: 0.09362\n",
      "Epoch 063: | Train Loss: 0.02978 | Validation Loss: 0.09480\n",
      "Epoch 064: | Train Loss: 0.02836 | Validation Loss: 0.09442\n",
      "Epoch 065: | Train Loss: 0.02820 | Validation Loss: 0.09350\n",
      "Epoch 066: | Train Loss: 0.02762 | Validation Loss: 0.09484\n",
      "Epoch 067: | Train Loss: 0.02737 | Validation Loss: 0.09356\n",
      "Epoch 068: | Train Loss: 0.02599 | Validation Loss: 0.09707\n",
      "Epoch 069: | Train Loss: 0.02659 | Validation Loss: 0.09208\n",
      "Epoch 070: | Train Loss: 0.02609 | Validation Loss: 0.09402\n",
      "Epoch 071: | Train Loss: 0.02476 | Validation Loss: 0.09349\n",
      "Epoch 072: | Train Loss: 0.02415 | Validation Loss: 0.09580\n",
      "Epoch 073: | Train Loss: 0.02442 | Validation Loss: 0.09773\n",
      "Epoch 074: | Train Loss: 0.02368 | Validation Loss: 0.09807\n",
      "Epoch 075: | Train Loss: 0.02279 | Validation Loss: 0.09860\n",
      "Epoch 076: | Train Loss: 0.02294 | Validation Loss: 0.10019\n",
      "Epoch 077: | Train Loss: 0.02243 | Validation Loss: 0.10412\n",
      "Epoch 078: | Train Loss: 0.02158 | Validation Loss: 0.09870\n",
      "Epoch 079: | Train Loss: 0.02199 | Validation Loss: 0.09929\n",
      "Epoch 080: | Train Loss: 0.02115 | Validation Loss: 0.09799\n",
      "Epoch 081: | Train Loss: 0.02086 | Validation Loss: 0.10159\n",
      "Epoch 082: | Train Loss: 0.02016 | Validation Loss: 0.10244\n",
      "Epoch 083: | Train Loss: 0.01917 | Validation Loss: 0.10176\n",
      "Epoch 084: | Train Loss: 0.01939 | Validation Loss: 0.12167\n",
      "Epoch 085: | Train Loss: 0.01941 | Validation Loss: 0.10917\n",
      "Epoch 086: | Train Loss: 0.01963 | Validation Loss: 0.10485\n",
      "Epoch 087: | Train Loss: 0.01897 | Validation Loss: 0.10404\n",
      "Epoch 088: | Train Loss: 0.01859 | Validation Loss: 0.10855\n",
      "Epoch 089: | Train Loss: 0.01874 | Validation Loss: 0.10506\n",
      "Epoch 090: | Train Loss: 0.01833 | Validation Loss: 0.10998\n",
      "Epoch 091: | Train Loss: 0.01713 | Validation Loss: 0.10271\n",
      "Epoch 092: | Train Loss: 0.01821 | Validation Loss: 0.10967\n",
      "Epoch 093: | Train Loss: 0.01721 | Validation Loss: 0.10259\n",
      "Epoch 094: | Train Loss: 0.01738 | Validation Loss: 0.10987\n",
      "Epoch 095: | Train Loss: 0.01707 | Validation Loss: 0.10702\n",
      "Epoch 096: | Train Loss: 0.01662 | Validation Loss: 0.10762\n",
      "Epoch 097: | Train Loss: 0.01595 | Validation Loss: 0.10802\n",
      "Epoch 098: | Train Loss: 0.01663 | Validation Loss: 0.11071\n",
      "Epoch 099: | Train Loss: 0.01607 | Validation Loss: 0.11085\n"
     ]
    }
   ],
   "source": [
    "# train model and save the parameters for least validation loss\n",
    "ceof = 20\n",
    "N_EPOCHS = 100\n",
    "valid_loss_min = np.Inf\n",
    "for e in range(0, N_EPOCHS):\n",
    "  train_loss = 0\n",
    "  valid_loss = 0\n",
    "  b_model_2.train()\n",
    "  for X_batch, y_batch,sll in train_loader:\n",
    "    X_batch = X_batch.float()\n",
    "    optimizer_2.zero_grad()\n",
    "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "    y_pred = b_model_2(X_batch)\n",
    "\n",
    "    loss = my_loss_2(y_pred, y_batch,sll)\n",
    "    #print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_2.step()\n",
    "    train_loss += loss.item()\n",
    "\n",
    "  b_model_2.eval() # prep model for evaluation\n",
    "  for data, target, sll in dev_loader:\n",
    "      data = data.float()\n",
    "\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = b_model_2(data)\n",
    "      # calculate the loss\n",
    "      loss = my_loss_2(output, target,sll)\n",
    "      valid_loss += loss.item()\n",
    "\n",
    "  if (valid_loss/len(dev_loader)) <= valid_loss_min:\n",
    "      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "      valid_loss_min/ceof,\n",
    "      valid_loss/(len(dev_loader)*ceof)) )\n",
    "      torch.save(b_model_2.state_dict(), path+'model_2.pt')\n",
    "      valid_loss_min = valid_loss/len(dev_loader)\n",
    "      \n",
    "\n",
    "  print(f'Epoch {e+0:03}: | Train Loss: {train_loss/(len(train_loader)*ceof):.5f} | Validation Loss: {valid_loss/(len(dev_loader)*ceof):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTHbDNfcDGf6",
    "outputId": "a173ad8d-e486-4ad4-a098-d4ab84d3b1f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state of model\n",
    "b_model_2.load_state_dict(torch.load(path+'model_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "x4lNQ6k9DbUC"
   },
   "outputs": [],
   "source": [
    "# predict dev data\n",
    "num_labels = len(train['tag'].unique())  \n",
    "all_prediction = predict(b_model_2, dev_loader)\n",
    "dev['pred'] = all_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "m378v95VDmQM"
   },
   "outputs": [],
   "source": [
    "# output to greedy.out\n",
    "# df = df.drop('column_name', 1)\n",
    "t2_output_df = dev.drop('all_features', 1)\n",
    "t2_output_list = t2_output_df.values.tolist()\n",
    "first_flag = True\n",
    "with open(path+'dev2.out', 'w') as f:\n",
    "    for each_output in t2_output_list:\n",
    "        idx = each_output[0]\n",
    "        word = each_output[1]\n",
    "        gold = each_output[2]\n",
    "        pred = each_output[3]\n",
    "\n",
    "        if idx==1:\n",
    "            if first_flag:\n",
    "                first_flag = False\n",
    "            else:\n",
    "                f.write('\\n')\n",
    "        try:\n",
    "          f.write(str(idx)+' '+word+' '+gold+' '+pred)\n",
    "          f.write('\\n')\n",
    "        except:\n",
    "          print(idx)\n",
    "          print(word)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "uvM-aoM9hLYx"
   },
   "outputs": [],
   "source": [
    "torch.save(b_model_2, path+'blstm2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P-sci-zwhLbK"
   },
   "outputs": [],
   "source": [
    "#################################################### Task 2 testing ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucoBWUI48RBK",
    "outputId": "e7948f8d-3dbd-488d-bbd3-d67160eb9c0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------Load GloVe------------------------------\n",
    "GloVe = pd.read_table(path+'glove.6B.100d.txt',header=None,sep=' |\\n', doublequote = False) \n",
    "GloVe[0] = GloVe[0].astype(str)\n",
    "GloVe_word_list= GloVe[0].tolist()\n",
    "GloVe.drop(GloVe.columns[0], axis=1, inplace=True)\n",
    "GloVe['target']= GloVe.values.tolist()\n",
    "GloVe_target_list = GloVe['target'].tolist()\n",
    "#[str(i) for i in lst]\n",
    "GloVe_word_list = [str(i) for i in GloVe_word_list]\n",
    "GloVe_word_index_dict = dict(zip(GloVe_word_list, GloVe_target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6Q8Eny_68RBL"
   },
   "outputs": [],
   "source": [
    "GloVe_lower_word_list = (map(lambda x: x.lower(), GloVe_word_list))\n",
    "GloVe_lowcase_word_index_dict = dict(zip(GloVe_lower_word_list, GloVe_target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "r9bggGWJ8RBM"
   },
   "outputs": [],
   "source": [
    "def add_features(s):\n",
    "  try:\n",
    "    feature_list = GloVe_word_index_dict[s]\n",
    "  except:\n",
    "    try:\n",
    "      feature_list = GloVe_lowcase_word_index_dict[s.lower()]\n",
    "    except:\n",
    "      if s[0].isdigit():\n",
    "        lookup_tensor = torch.tensor([word_index_dict['<unk_digit>']], dtype=torch.long)\n",
    "      elif s.isalnum():\n",
    "        lookup_tensor = torch.tensor([word_index_dict['<unk_alnum>']], dtype=torch.long)\n",
    "      else:\n",
    "        lookup_tensor = torch.tensor([word_index_dict['<unk>']], dtype=torch.long)\n",
    "      word_embed = embedding(lookup_tensor)\n",
    "      temp = word_embed.tolist()\n",
    "      feature_list = temp[0]\n",
    "    \n",
    "    #feature_list = [1]*100\n",
    "  return feature_list\n",
    "\n",
    "train['all_features'] = train['word'].apply(add_features) \n",
    "dev['all_features'] = dev['word'].apply(add_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4NKs2G4a8RBM"
   },
   "outputs": [],
   "source": [
    "# X\n",
    "train_index_list = train[\"index\"].tolist()\n",
    "train_feature_list = train[\"all_features\"].tolist()\n",
    "dev_index_list = dev[\"index\"].tolist()\n",
    "dev_feature_list = dev[\"all_features\"].tolist()\n",
    "X_train = getXY(train_index_list,train_feature_list)\n",
    "X_dev = getXY(dev_index_list,dev_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZKq55xoW8RBM"
   },
   "outputs": [],
   "source": [
    "# Y\n",
    "Y_train_df = pd.get_dummies(train.tag, prefix='y')\n",
    "Y_dev_df = pd.get_dummies(dev.tag, prefix='y')\n",
    "#Y_train_df['y_paddding'] = 0\n",
    "#Y_dev_df['y_paddding'] = 0\n",
    "Y_train_df['target']= Y_train_df.values.tolist()\n",
    "Y_dev_df['target']= Y_dev_df.values.tolist()\n",
    "train_target_list = Y_train_df[\"target\"].tolist()\n",
    "dev_target_list = Y_dev_df[\"target\"].tolist()\n",
    "#train_target_list\n",
    "Y_train = getXY(train_index_list,train_target_list)\n",
    "Y_dev = getXY(dev_index_list,dev_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dkSea8p08RBN"
   },
   "outputs": [],
   "source": [
    "train_dataset = list(zip(X_train,Y_train))\n",
    "dev_dataset = list(zip(X_dev,Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_LFzMlxM8RBN"
   },
   "outputs": [],
   "source": [
    "#################### data loader ####################\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,collate_fn=pad_collate, shuffle=False) \n",
    "dev_loader = DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, collate_fn=pad_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GrCpyjvj7ST1"
   },
   "outputs": [],
   "source": [
    "brnn_model_2 = torch.load(path+'blstm2.pt')\n",
    "brnn_model_2 = brnn_model_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "asEf6QKh8yIY"
   },
   "outputs": [],
   "source": [
    "# predict dev data\n",
    "num_labels = len(train['tag'].unique())  \n",
    "all_prediction = predict(brnn_model_2, dev_loader)\n",
    "dev['pred'] = all_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DkIQ9BSR8yIZ"
   },
   "outputs": [],
   "source": [
    "# output to greedy.out\n",
    "# df = df.drop('column_name', 1)\n",
    "t2_output_df = dev.drop('all_features', 1)\n",
    "t2_output_list = t2_output_df.values.tolist()\n",
    "first_flag = True\n",
    "with open(path+'dev2.out', 'w') as f:\n",
    "    for each_output in t2_output_list:\n",
    "        idx = each_output[0]\n",
    "        word = each_output[1]\n",
    "        gold = each_output[2]\n",
    "        pred = each_output[3]\n",
    "\n",
    "        if idx==1:\n",
    "            if first_flag:\n",
    "                first_flag = False\n",
    "            else:\n",
    "                f.write('\\n')\n",
    "        try:\n",
    "          f.write(str(idx)+' '+word+' '+gold+' '+pred)\n",
    "          f.write('\\n')\n",
    "        except:\n",
    "          print(idx)\n",
    "          print(word)\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "testing10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
